{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottn66/SURP/blob/main/src/DSPy_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwLGPYri2oS2"
      },
      "source": [
        "# 1. Define Your Task\n",
        "## Task: The goal is to detect bias in news articles based on their headlines.\n",
        "\n",
        "Expected Input/Output Behavior:\n",
        "\n",
        "Input: A news headline.\n",
        "Output: A prediction indicating whether the headline is biased.\n",
        "Examples:\n",
        "\n",
        "Input: \"178 Texas fires have torched more than 108,000 acres in the past week. But one major fire is now under control.\"\n",
        "Output: \"Not Biased\"\n",
        "Input: \"Government incompetence leads to devastating wildfires in Texas.\"\n",
        "Output: \"Biased\"\n",
        "Input: \"Climate change blamed for increasing wildfire frequency in Texas.\"\n",
        "Output: \"Not Biased\"\n",
        "Input: \"Wildfires in Texas: Another failure of the current administration.\"\n",
        "Output: \"Biased\"\n",
        "Quality and Cost Specs:\n",
        "\n",
        "Model Choice: GPT-3.5-turbo for initial development, potentially moving to a smaller, more cost-effective model like Llama2-13B-chat or Mixtral if resource constraints arise.\n",
        "Response Time: The system should respond quickly, ideally within a few seconds per headline.\n",
        "# 2. Define Your Pipeline\n",
        "## Pipeline Steps:\n",
        "\n",
        "Preprocess Input: Extract and clean the headline.\n",
        "Predict Bias: Use a language model to classify the headline as biased or not.\n",
        "Initial DSPy Program:\n",
        "\n",
        "```\n",
        "class HeadlineBiasDetection(dspy.Signature):\n",
        "    headline = dspy.InputField()\n",
        "    bias = dspy.OutputField()\n",
        "\n",
        "class HeadlineBiasClassifier(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.classify_bias = dspy.ChainOfThought(HeadlineBiasDetection)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        prediction = self.classify_bias(headline=inputs[\"headline\"])\n",
        "        return dspy.Prediction(label=prediction.bias)\n",
        "\n",
        "```\n",
        "\n",
        "# 3. Explore a Few Examples\n",
        "## Testing with Examples:\n",
        "\n",
        "Run the initial pipeline with a few examples using a large LM like GPT-3.5-turbo to understand the system's behavior.\n",
        "Record both successful and problematic cases to inform future optimization.\n",
        "Example Execution:\n",
        "\n",
        "```\n",
        "test_input = {\"headline\": \"178 Texas fires have torched more than 108,000 acres in the past week. But one major fire is now under control\"}\n",
        "prediction = compiled_model(test_input)\n",
        "print(f\"Prediction: {prediction.label}\")\n",
        "```\n",
        "\n",
        "\n",
        "# 4. Define Your Data\n",
        "## Training and Validation Data:\n",
        "\n",
        "Source: The data consists of news headlines and labels indicating bias.\n",
        "Size: Aim for 50-100 examples initially, with a goal of expanding to 300-500 examples.\n",
        "Dataset Preparation:\n",
        "```\n",
        "import pandas as pd\n",
        "df = pd.read_csv('cleaned_data_processed.csv')\n",
        "\n",
        "dataset = []\n",
        "for index, row in df.iterrows():\n",
        "    entry = {\n",
        "        \"inputs\": {\"headline\": row[\"headline\"]},\n",
        "        \"label\": row[\"Answer.bias-question\"]\n",
        "    }\n",
        "    dataset.append(entry)\n",
        "```\n",
        "# 5. Define Your Metric\n",
        "## Metric: Accuracy or exact match will be used to evaluate the performance of the bias detection.\n",
        "\n",
        "Metric Implementation:\n",
        "\n",
        "```\n",
        "def validate_prediction(example, pred, trace=None):\n",
        "    return example['label'] == pred.label\n",
        "```\n",
        "# 6. Collect Preliminary \"Zero-Shot\" Evaluations\n",
        "Initial Evaluation:\n",
        "\n",
        "Run the zero-shot pipeline on a few examples.\n",
        "Analyze the results and identify any immediate issues.\n",
        "# 7. Compile with a DSPy Optimizer\n",
        "Optimizer Selection:\n",
        "\n",
        "BootstrapFewShot: If you have very little data (e.g., 10 examples).\n",
        "BootstrapFewShotWithRandomSearch: If you have around 50 examples.\n",
        "MIPRO: If you have 300 or more examples.\n",
        "Optimization Execution:\n",
        "\n",
        "```\n",
        "from dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch, MIPRO\n",
        "```\n",
        "# Compile the model\n",
        "```\n",
        "teleprompter = BootstrapFewShot(metric=validate_prediction)\n",
        "compiled_model = teleprompter.compile(HeadlineBiasClassifier(), trainset=trainset)\n",
        "```\n",
        "8. Iterate\n",
        "Iterative Development:\n",
        "\n",
        "Revisit the task definition, data collection, metric, and model optimization steps.\n",
        "Continue refining each component incrementally based on feedback and performance evaluations.\n",
        "Project Summary\n",
        "Objective: Develop a system to detect bias in news headlines using a language model. The system takes a news headline as input and outputs a prediction of whether the headline is biased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVeUsejDlrPA",
        "outputId": "6a1e5df9-9922-443d-9fd9-61b7a14aa65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SURP'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 45 (delta 11), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (45/45), 3.58 MiB | 5.92 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n",
            "[Errno 2] No such file or directory: 'dat'\n",
            "/content\n",
            "sample_data  SURP\n",
            "--2024-07-22 19:19:31--  https://raw.githubusercontent.com/scottn66/SURP/main/dat/DSPy_dataset_formatted.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9496844 (9.1M) [text/plain]\n",
            "Saving to: ‘DSPy_dataset_formatted.csv’\n",
            "\n",
            "DSPy_dataset_format 100%[===================>]   9.06M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-07-22 19:19:32 (84.6 MB/s) - ‘DSPy_dataset_formatted.csv’ saved [9496844/9496844]\n",
            "\n",
            "--2024-07-22 19:19:32--  https://raw.githubusercontent.com/scottn66/SURP/main/dat/cleaned_data_processed.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9729350 (9.3M) [text/plain]\n",
            "Saving to: ‘cleaned_data_processed.csv’\n",
            "\n",
            "cleaned_data_proces 100%[===================>]   9.28M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-07-22 19:19:32 (83.9 MB/s) - ‘cleaned_data_processed.csv’ saved [9729350/9729350]\n",
            "\n",
            "cleaned_data_processed.csv  DSPy_dataset_formatted.csv\tsample_data  SURP\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/scottn66/SURP.git\n",
        "# Change directory to the repository\n",
        "%cd dat\n",
        "\n",
        "# List the files in the repository to verify\n",
        "!ls\n",
        "\n",
        "# Download a specific file from the repository\n",
        "!wget https://raw.githubusercontent.com/scottn66/SURP/main/dat/DSPy_dataset_formatted.csv\n",
        "!wget https://raw.githubusercontent.com/scottn66/SURP/main/dat/cleaned_data_processed.csv\n",
        "\n",
        "# List the files to verify the download\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aovqaU4jmxzS",
        "outputId": "806ca928-fa71-4283-f35d-4940dba5cba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dspy\n",
            "  Downloading dspy-0.1.5-py3-none-any.whl (1.3 kB)\n",
            "Collecting dspy-ai\n",
            "  Downloading dspy_ai-2.4.12-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dspy_ai-2.4.5-py3-none-any.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.5/197.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff~=2.2.1 (from dspy-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting joblib~=1.3.2 (from dspy-ai)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=0.28.1 (from dspy-ai)\n",
            "  Downloading openai-1.37.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (2.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (2024.5.15)\n",
            "Collecting ujson (from dspy-ai)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (4.66.4)\n",
            "Collecting datasets<3.0.0,~=2.14.6 (from dspy-ai)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (2.31.0)\n",
            "Collecting optuna (from dspy-ai)\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==2.5.0 (from dspy-ai)\n",
            "  Downloading pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.5/407.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.5.0->dspy-ai) (0.7.0)\n",
            "Collecting pydantic-core==2.14.1 (from pydantic==2.5.0->dspy-ai)\n",
            "  Downloading pydantic_core-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.5.0->dspy-ai) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=0.28.1->dspy-ai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai) (2024.7.4)\n",
            "Collecting alembic>=1.5.0 (from optuna->dspy-ai)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna->dspy-ai)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->dspy-ai) (2.0.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy-ai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy-ai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy-ai) (2024.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->dspy-ai)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=0.28.1->dspy-ai) (1.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (4.0.3)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets<3.0.0,~=2.14.6->dspy-ai) (3.15.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dspy-ai) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->dspy-ai) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna->dspy-ai) (2.1.5)\n",
            "Installing collected packages: xxhash, ujson, pydantic-core, Mako, joblib, h11, dill, colorlog, backoff, pydantic, multiprocess, httpcore, alembic, optuna, httpx, openai, datasets, dspy-ai, dspy\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.20.1\n",
            "    Uninstalling pydantic_core-2.20.1:\n",
            "      Successfully uninstalled pydantic_core-2.20.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.2\n",
            "    Uninstalling pydantic-2.8.2:\n",
            "      Successfully uninstalled pydantic-2.8.2\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 backoff-2.2.1 colorlog-6.8.2 datasets-2.14.7 dill-0.3.7 dspy-0.1.5 dspy-ai-2.4.5 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 joblib-1.3.2 multiprocess-0.70.15 openai-1.37.0 optuna-3.6.1 pydantic-2.5.0 pydantic-core-2.14.1 ujson-5.10.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install dspy dspy-ai\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_TOKEN = getpass()\n",
        "\n",
        "os.environ[\"OPENAI_API_TOKEN\"] = OPENAI_API_TOKEN\n",
        "# openai.api_key =\n",
        "\n",
        "import dspy\n",
        "\n",
        "turbo = dspy.OpenAI(model='gpt-3.5-turbo', max_tokens=24)\n",
        "dspy.settings.configure(lm=turbo)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX7BlfIsymk0"
      },
      "source": [
        "Just predicting bias-question from headline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByDUIiDX5e9e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "dataframe = pd.read_csv('cleaned_data_processed.csv')\n",
        "\n",
        "#sampling\n",
        "n=1500\n",
        "random_seed = 42\n",
        "df = dataframe.sample(n=n, random_state=random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnF1jXcJYUO0"
      },
      "source": [
        "# Class-based DSPy Signature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO6OlXmY61Jo"
      },
      "outputs": [],
      "source": [
        "# My initially simpler signature left for reference. Excluded content.\n",
        "# Find latest work under MIPRO Optimizer in Table of Contents. If you're new to DSPy, continue reading\n",
        "\n",
        "# class Bias(dspy.Signature):\n",
        "#     \"\"\"Predict, based on the person's demographics, a subjective classification of a headline as is-biased or is-not-biased\"\"\"\n",
        "#     age = dspy.InputField()\n",
        "#     gender = dspy.InputField()\n",
        "#     political_party = dspy.InputField()\n",
        "#     country = dspy.InputField()\n",
        "#     language = dspy.InputField()\n",
        "#     facebook_hours = dspy.InputField()\n",
        "#     instagram_hours = dspy.InputField()\n",
        "#     twitter_hours = dspy.InputField()\n",
        "#     reddit_hours = dspy.InputField()\n",
        "#     news_outlet = dspy.InputField()\n",
        "#     headline = dspy.InputField()\n",
        "#     bias_label = dspy.OutputField()\n",
        "\n",
        "\n",
        "class Bias(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Predict whether a news headline is perceived as biased or not based on a person's demographics and social media usage.\n",
        "\n",
        "    The input features include:\n",
        "    - age: The age of the participant.\n",
        "    - gender: The gender of the participant.\n",
        "    - political_party: The political party affiliation of the participant.\n",
        "    - country: The country where the participant resides.\n",
        "    - language: The primary language spoken by the participant.\n",
        "    - facebook_hours: The number of hours the participant spends on Facebook per day.\n",
        "    - instagram_hours: The number of hours the participant spends on Instagram per day.\n",
        "    - twitter_hours: The number of hours the participant spends on Twitter per day.\n",
        "    - reddit_hours: The number of hours the participant spends on Reddit per day.\n",
        "    - news_outlet: The news outlet that published the headline.\n",
        "    - headline: The headline of the news article.\n",
        "\n",
        "    The output is:\n",
        "    - bias_label: The participant's subjective classification of the headline as 'is-biased' or 'is-not-biased'.\n",
        "    \"\"\"\n",
        "    age = dspy.InputField(desc=\"The age of the participant.\")\n",
        "    gender = dspy.InputField(desc=\"The gender of the participant.\")\n",
        "    political_party = dspy.InputField(desc=\"The political party affiliation of the participant.\")\n",
        "    country = dspy.InputField(desc=\"The country where the participant resides.\")\n",
        "    language = dspy.InputField(desc=\"The primary language spoken by the participant.\")\n",
        "    facebook_hours = dspy.InputField(desc=\"The number of hours the participant spends on Facebook per day.\")\n",
        "    instagram_hours = dspy.InputField(desc=\"The number of hours the participant spends on Instagram per day.\")\n",
        "    twitter_hours = dspy.InputField(desc=\"The number of hours the participant spends on Twitter per day.\")\n",
        "    reddit_hours = dspy.InputField(desc=\"The number of hours the participant spends on Reddit per day.\")\n",
        "    news_outlet = dspy.InputField(desc=\"The news outlet that published the headline.\")\n",
        "    headline = dspy.InputField(desc=\"The headline of the news article.\")\n",
        "    bias_label = dspy.OutputField(desc=\"The participant's subjective classification of the headline as 'is-biased' or 'is-not-biased'.\")\n",
        "\n",
        "# Define a module for bias prediction\n",
        "class BiasPrediction(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.predict_bias = dspy.ChainOfThought(Bias)\n",
        "\n",
        "    def forward(self, age, gender, political_party, country, language, facebook_hours, instagram_hours, twitter_hours, reddit_hours, news_outlet, headline):\n",
        "        prediction = self.predict_bias(\n",
        "            age=age,\n",
        "            gender=gender,\n",
        "            political_party=political_party,\n",
        "            country=country,\n",
        "            language=language,\n",
        "            facebook_hours=facebook_hours,\n",
        "            instagram_hours=instagram_hours,\n",
        "            twitter_hours=twitter_hours,\n",
        "            reddit_hours=reddit_hours,\n",
        "            news_outlet=news_outlet,\n",
        "            headline=headline\n",
        "        )\n",
        "        return dspy.Prediction(bias_label=prediction.bias_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgANJ2gsYfN-"
      },
      "source": [
        "# Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUGyGsbcY7r0"
      },
      "outputs": [],
      "source": [
        "# a Module abstracts a prompting technique - Generalized to handle any DSPy Signature\n",
        "# A DSPy Module has learnable parameters (pieces of the prompt and LM weights) and can be invoked to process inputs and return outputs\n",
        "# Multiple modules can be composed into bigger modules (programs). Inspired by NN modules, DSPy applies this to LM programs\n",
        "\n",
        "# headline, bq = get_random_headline()\n",
        "# # alternate: headline, bias_question, confidence_level\n",
        "# classify = dspy.Predict(Bias)\n",
        "# output = classify(headline=headline)\n",
        "# print(headline, output, bq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qaI447oZ2QO"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lujUQJk-Z4E5",
        "outputId": "6c288e20-544f-4d05-e02a-2d4927f7d21f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "400 100\n",
            "still need to consider two datasets segmented on the political party, training two models on the respective political party and then using those model predictions.\n"
          ]
        }
      ],
      "source": [
        "# DSPy Example Objects - similar to Python dicts +added utilities\n",
        "# Examples represent items in your training set and test set\n",
        "#DSPy Modules will return values of the type *Prediction*, which is a special sub-class of Example.\n",
        "# you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type *Example*\n",
        "\n",
        "# object_ = Example(field1=value1, field2=value2, field3=value3, ...)\n",
        "\n",
        "# trainset = [dspy.Example(report=\"LONG REPORT 1\", summary=\"short summary 1\"), ...]\n",
        "\n",
        "# Shuffle and split the dataset into train, validation, and hold-out sets\n",
        "# train_dev_set, holdout_set = train_test_split(df, test_size=0.1, random_state=42)\n",
        "# train_set, dev_set = train_test_split(train_dev_set, test_size=0.2, random_state=42)\n",
        "# hb_pair = dspy.Example(headline=headline, bias_label=bq)\n",
        "\n",
        "# print(hb_pair)\n",
        "# print(hb_pair.headline)\n",
        "# print(hb_pair.bias_label)\n",
        "\n",
        "# create training and validation sets\n",
        "train_size = int(0.8* len(df))\n",
        "\n",
        "# Create training and validation sets using list comprehensions\n",
        "trainset = [dspy.Example(age = entry['Answer.age'],\n",
        "                         gender = entry['Answer.gender'],\n",
        "                         political_party = entry['Answer.politics'],\n",
        "                         country = entry['Answer.country'],\n",
        "                         language = entry['Answer.language1'],\n",
        "                         facebook_hours = entry['Answer.facebook-hours'],\n",
        "                         instagram_hours = entry['Answer.instagram-hours'],\n",
        "                         twitter_hours = entry['Answer.twitter-hours'],\n",
        "                         reddit_hours = entry['Answer.reddit-hours'],\n",
        "                         news_outlet = entry['Answer.newsOutlet'],\n",
        "                         headline=entry['headline'],\n",
        "                         bias_label=entry['Answer.bias-question']).with_inputs(\n",
        "        'age',\n",
        "        'gender',\n",
        "        'country',\n",
        "        'facebook_hours',\n",
        "        'instagram_hours',\n",
        "        'twitter_hours',\n",
        "        'reddit_hours',\n",
        "        'language',\n",
        "        'news_outlet',\n",
        "        'political_party',\n",
        "        'url',\n",
        "        'headline'\n",
        "    ) for index, entry in df.iloc[:train_size].iterrows()]\n",
        "\n",
        "devset = [dspy.Example(age = entry['Answer.age'],\n",
        "                         gender = entry['Answer.gender'],\n",
        "                         political_party = entry['Answer.politics'],\n",
        "                         country = entry['Answer.country'],\n",
        "                         language = entry['Answer.language1'],\n",
        "                         facebook_hours = entry['Answer.facebook-hours'],\n",
        "                         instagram_hours = entry['Answer.instagram-hours'],\n",
        "                         twitter_hours = entry['Answer.twitter-hours'],\n",
        "                         reddit_hours = entry['Answer.reddit-hours'],\n",
        "                         news_outlet = entry['Answer.newsOutlet'],\n",
        "                         headline=entry['headline'],\n",
        "                         bias_label=entry['Answer.bias-question']).with_inputs(\n",
        "        'age',\n",
        "        'gender',\n",
        "        'country',\n",
        "        'facebook_hours',\n",
        "        'instagram_hours',\n",
        "        'twitter_hours',\n",
        "        'reddit_hours',\n",
        "        'language',\n",
        "        'news_outlet',\n",
        "        'political_party',\n",
        "        'url',\n",
        "        'headline'\n",
        "    ) for index, entry in df.iloc[train_size:].iterrows()]\n",
        "\n",
        "print(len(trainset), len(devset))\n",
        "print('still need to consider two datasets segmented on the political party, training two models on the respective political party and then using those model predictions. in a composite model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gmsqIHc5gp"
      },
      "source": [
        "# Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f2qAGAZc7aE"
      },
      "outputs": [],
      "source": [
        "# Metric is a function that takes examples from your data and take the output of your system and return a score that quantifies how good the output is.\n",
        "# Confidence interval and boolean\n",
        "\n",
        "# simple example of a metric\n",
        "# def validate_answer(example, pred, trace=None):\n",
        "#     return example.answer.lower() == pred.answer.lower()\n",
        "\n",
        "# # more complex metric\n",
        "# def validate_context_and_answer(example, pred, trace=None):\n",
        "#     # check the gold label and the predicted answer are the same\n",
        "#     answer_match = example.answer.lower() == pred.answer.lower()\n",
        "\n",
        "#     # check the predicted answer comes from one of the retrieved contexts\n",
        "#     context_match = any((pred.answer.lower() in c) for c in pred.context)\n",
        "\n",
        "#     if trace is None: # if we're doing evaluation or optimization\n",
        "#         return (answer_match + context_match) / 2.0\n",
        "#     else: # if we're doing bootstrapping, i.e. self-generating good demonstrations of each step\n",
        "#         return answer_match and context_match\n",
        "\n",
        "# My metric for headline: bias\n",
        "def validate_answer(example, pred, trace=None):\n",
        "    return example.bias_label.lower() == pred.bias_label.lower()\n",
        "\n",
        "def validate_response(example, pred, trace=None):\n",
        "    return dspy.evaluate.answer_exact_match(example.bias_label, pred.bias_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCHi2H9HquTA"
      },
      "source": [
        "Initial dspy.Predict from headlines alone was 44.6% using gpt-3.5-turbo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N7GsTLstpAd"
      },
      "outputs": [],
      "source": [
        "class SubjectiveBiasLabel(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.generate_prediction = dspy.Predict(Bias)\n",
        "\n",
        "    def forward(self, age, gender, political_party, country, language, facebook_hours, instagram_hours, twitter_hours, reddit_hours, news_outlet, headline):\n",
        "        prediction = self.generate_prediction(\n",
        "            age=age,\n",
        "            gender=gender,\n",
        "            political_party=political_party,\n",
        "            country=country,\n",
        "            language=language,\n",
        "            facebook_hours=facebook_hours,\n",
        "            instagram_hours=instagram_hours,\n",
        "            twitter_hours=twitter_hours,\n",
        "            reddit_hours=reddit_hours,\n",
        "            news_outlet=news_outlet,\n",
        "            headline=headline\n",
        "        )\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWGwmRqZrLty"
      },
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "QkUEoWEnrIhP",
        "outputId": "9180e873-785e-47b7-dfcf-50c14219338f"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "MIPRO.compile() got an unexpected keyword argument 'num_batches'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c3baae705993>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Compile the module with the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# optimized_program = teleprompter.compile(BiasPrediction(), trainset=trainset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mbayesian_optimized_program\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesian_teleprompter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBiasPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bootstrapped_demos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_labeled_demos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: MIPRO.compile() got an unexpected keyword argument 'num_batches'"
          ]
        }
      ],
      "source": [
        "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, MIPRO\n",
        "\n",
        "# config = dict(max_bootstrapped_demos=4, max_labeled_demos=4, num_candidate_programs=5, num_threads=4)\n",
        "\n",
        "# teleprompter = BootstrapFewShotWithRandomSearch(metric=validate_answer, **config)\n",
        "\n",
        "mipro_config = {\n",
        "    'metric': validate_answer,\n",
        "    'prompt_model': dspy.OpenAI(model='gpt-3.5-turbo'),\n",
        "    'task_model': dspy.OpenAI(model='gpt-3.5-turbo'),\n",
        "    'num_candidates': 10,\n",
        "    'init_temperature': 1.0,\n",
        "    'verbose': True,\n",
        "    'track_stats': True,\n",
        "    'view_data_batch_size': 10\n",
        "}\n",
        "eval_kwargs = dict(num_threads=3, display_progress=True, display_table=0)\n",
        "Bayesian_teleprompter = MIPRO(**mipro_config)\n",
        "\n",
        "# Compile the module with the training data\n",
        "# optimized_program = teleprompter.compile(BiasPrediction(), trainset=trainset)\n",
        "bayesian_optimized_program = Bayesian_teleprompter.compile(BiasPrediction(), trainset=trainset, num_trials=3, max_bootstrapped_demos=1, max_labeled_demos=2, eval_kwargs=eval_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vV_HUGijJ5eE",
        "outputId": "8c881fe0-12b5-47bc-a80d-44bda23c1f53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 104 / 220  (47.3):  50%|████▉     | 220/444 [00:17<00:24,  9.14it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89934, Requested 497. Please try again in 287ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89990, Requested 496. Please try again in 324ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 104 / 221  (47.1):  50%|████▉     | 220/444 [00:17<00:24,  9.14it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89914, Requested 508. Please try again in 281ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 104 / 222  (46.8):  50%|█████     | 222/444 [00:18<00:37,  5.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 104 / 223  (46.6):  50%|█████     | 223/444 [00:18<00:41,  5.34it/s]INFO:backoff:Backing off request(...) for 1.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89926, Requested 497. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 104 / 224  (46.4):  50%|█████     | 224/444 [00:18<00:49,  4.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 105 / 226  (46.5):  51%|█████     | 226/444 [00:19<00:59,  3.69it/s]INFO:backoff:Backing off request(...) for 1.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89750, Requested 508. Please try again in 172ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 106 / 227  (46.7):  51%|█████     | 227/444 [00:19<01:04,  3.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.8 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 107 / 228  (46.9):  51%|█████▏    | 228/444 [00:20<01:08,  3.16it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89925, Requested 497. Please try again in 281ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89928, Requested 500. Please try again in 285ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 108 / 229  (47.2):  52%|█████▏    | 229/444 [00:20<01:06,  3.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 110 / 232  (47.4):  52%|█████▏    | 232/444 [00:21<01:13,  2.88it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89905, Requested 504. Please try again in 272ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 111 / 233  (47.6):  52%|█████▏    | 233/444 [00:22<01:17,  2.71it/s]INFO:backoff:Backing off request(...) for 3.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89918, Requested 508. Please try again in 284ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 112 / 234  (47.9):  53%|█████▎    | 234/444 [00:22<01:10,  2.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.7 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 113 / 237  (47.7):  53%|█████▎    | 236/444 [00:23<01:10,  2.94it/s]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89916, Requested 505. Please try again in 280ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 114 / 239  (47.7):  54%|█████▍    | 239/444 [00:23<00:57,  3.56it/s]INFO:backoff:Backing off request(...) for 1.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89908, Requested 505. Please try again in 275ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 115 / 240  (47.9):  54%|█████▍    | 240/444 [00:24<01:02,  3.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 117 / 243  (48.1):  55%|█████▍    | 243/444 [00:25<01:07,  2.96it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89939, Requested 500. Please try again in 292ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 118 / 245  (48.2):  55%|█████▌    | 245/444 [00:25<01:10,  2.83it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89935, Requested 493. Please try again in 285ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 118 / 246  (48.0):  55%|█████▌    | 246/444 [00:26<01:06,  2.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 6.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89865, Requested 508. Please try again in 248ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 118 / 247  (47.8):  56%|█████▌    | 247/444 [00:26<01:07,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 6.0 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 119 / 248  (48.0):  56%|█████▌    | 248/444 [00:26<01:08,  2.87it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89890, Requested 508. Please try again in 265ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 120 / 251  (47.8):  57%|█████▋    | 251/444 [00:28<01:03,  3.05it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89928, Requested 508. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 120 / 252  (47.6):  57%|█████▋    | 252/444 [00:28<01:02,  3.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 120 / 253  (47.4):  57%|█████▋    | 253/444 [00:28<01:09,  2.73it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89945, Requested 491. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 121 / 255  (47.5):  57%|█████▋    | 255/444 [00:29<01:02,  3.03it/s]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89892, Requested 504. Please try again in 264ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 122 / 256  (47.7):  58%|█████▊    | 256/444 [00:29<01:02,  2.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 122 / 257  (47.5):  58%|█████▊    | 257/444 [00:30<01:05,  2.86it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89930, Requested 494. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 125 / 260  (48.1):  59%|█████▊    | 260/444 [00:31<01:03,  2.91it/s]INFO:backoff:Backing off request(...) for 1.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89915, Requested 494. Please try again in 272ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 126 / 261  (48.3):  59%|█████▉    | 261/444 [00:31<01:05,  2.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 129 / 264  (48.9):  59%|█████▉    | 264/444 [00:32<01:07,  2.65it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89920, Requested 502. Please try again in 281ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 130 / 265  (49.1):  60%|█████▉    | 265/444 [00:32<00:59,  3.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 130 / 266  (48.9):  60%|█████▉    | 266/444 [00:33<01:07,  2.64it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89865, Requested 505. Please try again in 246ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 131 / 267  (49.1):  60%|██████    | 267/444 [00:33<00:58,  3.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 131 / 268  (48.9):  60%|██████    | 268/444 [00:34<00:58,  2.99it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89919, Requested 507. Please try again in 284ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89914, Requested 507. Please try again in 280ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 132 / 269  (49.1):  61%|██████    | 269/444 [00:34<01:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89919, Requested 503. Please try again in 281ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 132 / 270  (48.9):  61%|██████    | 270/444 [00:34<00:59,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 133 / 271  (49.1):  61%|██████    | 271/444 [00:35<01:06,  2.60it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89925, Requested 508. Please try again in 288ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 133 / 272  (48.9):  61%|██████▏   | 272/444 [00:35<00:58,  2.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89913, Requested 507. Please try again in 280ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 134 / 273  (49.1):  61%|██████▏   | 273/444 [00:35<01:04,  2.64it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89920, Requested 508. Please try again in 285ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 135 / 274  (49.3):  62%|██████▏   | 274/444 [00:36<00:58,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 135 / 277  (48.7):  62%|██████▏   | 277/444 [00:37<01:04,  2.60it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89929, Requested 499. Please try again in 285ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 136 / 278  (48.9):  63%|██████▎   | 278/444 [00:37<00:56,  2.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 136 / 279  (48.7):  63%|██████▎   | 279/444 [00:38<01:01,  2.69it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89932, Requested 507. Please try again in 292ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 137 / 281  (48.8):  63%|██████▎   | 281/444 [00:38<00:54,  3.00it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89925, Requested 501. Please try again in 284ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 137 / 282  (48.6):  64%|██████▎   | 282/444 [00:38<00:54,  2.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 137 / 283  (48.4):  64%|██████▎   | 283/444 [00:39<01:00,  2.67it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89893, Requested 497. Please try again in 260ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 1.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89881, Requested 507. Please try again in 258ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 1.6 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 139 / 285  (48.8):  64%|██████▍   | 285/444 [00:40<00:55,  2.86it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89938, Requested 491. Please try again in 286ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 139 / 286  (48.6):  64%|██████▍   | 286/444 [00:40<00:49,  3.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 139 / 287  (48.4):  65%|██████▍   | 287/444 [00:40<00:53,  2.93it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89912, Requested 491. Please try again in 268ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 140 / 288  (48.6):  65%|██████▍   | 288/444 [00:41<00:53,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 141 / 289  (48.8):  65%|██████▌   | 289/444 [00:41<00:58,  2.67it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89939, Requested 490. Please try again in 286ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 2.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89929, Requested 491. Please try again in 280ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 2.8 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 141 / 292  (48.3):  66%|██████▌   | 292/444 [00:42<00:51,  2.92it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89922, Requested 495. Please try again in 278ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 143 / 295  (48.5):  66%|██████▋   | 295/444 [00:43<00:50,  2.97it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89926, Requested 508. Please try again in 289ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 143 / 296  (48.3):  67%|██████▋   | 296/444 [00:43<00:50,  2.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 144 / 297  (48.5):  67%|██████▋   | 297/444 [00:44<00:54,  2.68it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89918, Requested 508. Please try again in 284ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 144 / 299  (48.2):  67%|██████▋   | 299/444 [00:44<00:48,  2.99it/s]INFO:backoff:Backing off request(...) for 1.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89894, Requested 508. Please try again in 268ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 145 / 300  (48.3):  68%|██████▊   | 300/444 [00:45<00:50,  2.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 146 / 301  (48.5):  68%|██████▊   | 301/444 [00:45<00:48,  2.95it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89928, Requested 497. Please try again in 283ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 147 / 302  (48.7):  68%|██████▊   | 302/444 [00:46<00:53,  2.64it/s]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89919, Requested 492. Please try again in 274ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 150 / 305  (49.2):  69%|██████▊   | 305/444 [00:47<00:50,  2.77it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89918, Requested 502. Please try again in 280ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 151 / 306  (49.3):  69%|██████▉   | 306/444 [00:47<00:49,  2.81it/s]INFO:backoff:Backing off request(...) for 0.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89938, Requested 503. Please try again in 294ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 2.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89936, Requested 508. Please try again in 296ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 2.8 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 153 / 308  (49.7):  69%|██████▉   | 308/444 [00:48<00:44,  3.09it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89917, Requested 502. Please try again in 279ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 153 / 309  (49.5):  70%|██████▉   | 309/444 [00:48<00:46,  2.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 153 / 310  (49.4):  70%|██████▉   | 310/444 [00:48<00:49,  2.68it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89931, Requested 496. Please try again in 284ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 153 / 311  (49.2):  70%|██████▉   | 310/444 [00:49<00:49,  2.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 153 / 312  (49.0):  70%|███████   | 312/444 [00:49<00:45,  2.89it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89941, Requested 495. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 154 / 313  (49.2):  70%|███████   | 313/444 [00:49<00:47,  2.78it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89917, Requested 496. Please try again in 275ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 154 / 316  (48.7):  71%|███████   | 316/444 [00:51<00:46,  2.73it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89929, Requested 503. Please try again in 288ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 5.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89921, Requested 508. Please try again in 286ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 155 / 317  (48.9):  71%|███████▏  | 317/444 [00:51<00:42,  3.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 5.0 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 155 / 318  (48.7):  72%|███████▏  | 318/444 [00:51<00:46,  2.69it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89924, Requested 505. Please try again in 286ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 158 / 322  (49.1):  73%|███████▎  | 322/444 [00:53<00:41,  2.92it/s]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89910, Requested 505. Please try again in 276ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 159 / 323  (49.2):  73%|███████▎  | 323/444 [00:53<00:42,  2.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 160 / 324  (49.4):  73%|███████▎  | 324/444 [00:53<00:42,  2.83it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89929, Requested 506. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 161 / 325  (49.5):  73%|███████▎  | 325/444 [00:54<00:41,  2.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 162 / 326  (49.7):  73%|███████▎  | 326/444 [00:54<00:45,  2.60it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89873, Requested 508. Please try again in 254ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 162 / 327  (49.5):  74%|███████▎  | 327/444 [00:54<00:40,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 162 / 328  (49.4):  74%|███████▍  | 328/444 [00:55<00:42,  2.73it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89923, Requested 492. Please try again in 276ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 162 / 331  (48.9):  75%|███████▍  | 331/444 [00:56<00:41,  2.73it/s]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89930, Requested 494. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 163 / 332  (49.1):  75%|███████▍  | 332/444 [00:56<00:38,  2.95it/s]INFO:backoff:Backing off request(...) for 4.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89920, Requested 508. Please try again in 285ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 4.9 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 163 / 333  (48.9):  75%|███████▌  | 333/444 [00:57<00:39,  2.78it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89935, Requested 504. Please try again in 292ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89931, Requested 501. Please try again in 288ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 164 / 337  (48.7):  76%|███████▌  | 337/444 [00:58<00:38,  2.75it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89935, Requested 509. Please try again in 296ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 165 / 338  (48.8):  76%|███████▌  | 338/444 [00:58<00:37,  2.80it/s]INFO:backoff:Backing off request(...) for 1.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89933, Requested 504. Please try again in 291ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 167 / 341  (49.0):  77%|███████▋  | 341/444 [00:59<00:37,  2.78it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89937, Requested 509. Please try again in 297ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 168 / 344  (48.8):  77%|███████▋  | 344/444 [01:00<00:33,  3.01it/s]INFO:backoff:Backing off request(...) for 2.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89912, Requested 509. Please try again in 280ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 168 / 345  (48.7):  77%|███████▋  | 344/444 [01:01<00:33,  3.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.9 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 168 / 346  (48.6):  78%|███████▊  | 346/444 [01:01<00:33,  2.89it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89937, Requested 506. Please try again in 295ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 168 / 347  (48.4):  78%|███████▊  | 347/444 [01:01<00:35,  2.75it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89873, Requested 507. Please try again in 253ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 169 / 348  (48.6):  78%|███████▊  | 348/444 [01:02<00:32,  2.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 169 / 349  (48.4):  79%|███████▊  | 349/444 [01:02<00:34,  2.74it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89923, Requested 508. Please try again in 287ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 170 / 350  (48.6):  79%|███████▉  | 350/444 [01:03<00:34,  2.76it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89927, Requested 506. Please try again in 288ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 170 / 351  (48.4):  79%|███████▉  | 351/444 [01:03<00:31,  2.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 170 / 353  (48.2):  80%|███████▉  | 353/444 [01:04<00:31,  2.86it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89861, Requested 508. Please try again in 246ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 170 / 354  (48.0):  80%|███████▉  | 354/444 [01:04<00:31,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 5.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89898, Requested 509. Please try again in 271ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 171 / 355  (48.2):  80%|███████▉  | 355/444 [01:04<00:30,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 5.0 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89903, Requested 496. Please try again in 266ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 172 / 358  (48.0):  81%|████████  | 358/444 [01:05<00:30,  2.79it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89920, Requested 508. Please try again in 285ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 172 / 359  (47.9):  81%|████████  | 359/444 [01:06<00:29,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 173 / 361  (47.9):  81%|████████▏ | 361/444 [01:06<00:30,  2.71it/s]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89940, Requested 498. Please try again in 292ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 174 / 362  (48.1):  82%|████████▏ | 362/444 [01:07<00:27,  2.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 3.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89915, Requested 508. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 174 / 363  (47.9):  82%|████████▏ | 363/444 [01:07<00:28,  2.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.7 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 179 / 370  (48.4):  83%|████████▎ | 370/444 [01:10<00:24,  3.04it/s]INFO:backoff:Backing off request(...) for 9.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89899, Requested 509. Please try again in 272ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 9.7 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 179 / 371  (48.2):  84%|████████▎ | 371/444 [01:10<00:26,  2.81it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89930, Requested 506. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 179 / 372  (48.1):  84%|████████▍ | 372/444 [01:10<00:23,  3.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 179 / 374  (47.9):  84%|████████▍ | 374/444 [01:11<00:24,  2.88it/s]INFO:backoff:Backing off request(...) for 15.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89919, Requested 508. Please try again in 284ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 15.6 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 180 / 386  (46.6):  87%|████████▋ | 386/444 [01:15<00:21,  2.72it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89926, Requested 506. Please try again in 288ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 180 / 387  (46.5):  87%|████████▋ | 387/444 [01:16<00:19,  2.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 183 / 397  (46.1):  89%|████████▉ | 397/444 [01:19<00:17,  2.62it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89929, Requested 494. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 185 / 401  (46.1):  90%|█████████ | 401/444 [01:20<00:14,  2.96it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89879, Requested 494. Please try again in 248ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 186 / 402  (46.3):  91%|█████████ | 402/444 [01:21<00:14,  2.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 187 / 404  (46.3):  91%|█████████ | 404/444 [01:22<00:13,  2.92it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89907, Requested 509. Please try again in 277ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 187 / 405  (46.2):  91%|█████████ | 405/444 [01:22<00:13,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 188 / 409  (46.0):  92%|█████████▏| 409/444 [01:23<00:11,  3.00it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89927, Requested 509. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 189 / 410  (46.1):  92%|█████████▏| 410/444 [01:24<00:11,  2.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 189 / 413  (45.8):  93%|█████████▎| 413/444 [01:25<00:10,  2.88it/s]INFO:backoff:Backing off request(...) for 3.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89902, Requested 509. Please try again in 274ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 190 / 414  (45.9):  93%|█████████▎| 414/444 [01:25<00:10,  2.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.0 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 190 / 415  (45.8):  93%|█████████▎| 415/444 [01:25<00:11,  2.64it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89865, Requested 504. Please try again in 246ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 190 / 416  (45.7):  94%|█████████▎| 416/444 [01:26<00:09,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 193 / 420  (46.0):  95%|█████████▍| 420/444 [01:27<00:08,  2.95it/s]INFO:backoff:Backing off request(...) for 16.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89916, Requested 508. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 194 / 421  (46.1):  95%|█████████▍| 421/444 [01:28<00:07,  2.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 16.8 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 194 / 423  (45.9):  95%|█████████▌| 423/444 [01:28<00:07,  2.78it/s]INFO:backoff:Backing off request(...) for 1.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89920, Requested 509. Please try again in 286ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.3 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 196 / 428  (45.8):  96%|█████████▋| 428/444 [01:30<00:05,  3.00it/s]INFO:backoff:Backing off request(...) for 8.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89915, Requested 509. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 8.1 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 196 / 429  (45.7):  97%|█████████▋| 429/444 [01:30<00:05,  2.68it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89900, Requested 507. Please try again in 271ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 199 / 434  (45.9):  98%|█████████▊| 434/444 [01:32<00:03,  3.02it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89869, Requested 494. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 202 / 438  (46.1):  99%|█████████▊| 438/444 [01:34<00:02,  2.79it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89930, Requested 494. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 207 / 444  (46.6): 100%|██████████| 444/444 [01:44<00:00,  4.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 207 / 444  (46.6%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_57d1c th {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_57d1c td {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_57d1c_row0_col0, #T_57d1c_row0_col1, #T_57d1c_row0_col2, #T_57d1c_row0_col3, #T_57d1c_row0_col4, #T_57d1c_row0_col5, #T_57d1c_row0_col6, #T_57d1c_row0_col7, #T_57d1c_row0_col8, #T_57d1c_row0_col9, #T_57d1c_row0_col10, #T_57d1c_row0_col11, #T_57d1c_row0_col12, #T_57d1c_row0_col13, #T_57d1c_row1_col0, #T_57d1c_row1_col1, #T_57d1c_row1_col2, #T_57d1c_row1_col3, #T_57d1c_row1_col4, #T_57d1c_row1_col5, #T_57d1c_row1_col6, #T_57d1c_row1_col7, #T_57d1c_row1_col8, #T_57d1c_row1_col9, #T_57d1c_row1_col10, #T_57d1c_row1_col11, #T_57d1c_row1_col12, #T_57d1c_row1_col13, #T_57d1c_row2_col0, #T_57d1c_row2_col1, #T_57d1c_row2_col2, #T_57d1c_row2_col3, #T_57d1c_row2_col4, #T_57d1c_row2_col5, #T_57d1c_row2_col6, #T_57d1c_row2_col7, #T_57d1c_row2_col8, #T_57d1c_row2_col9, #T_57d1c_row2_col10, #T_57d1c_row2_col11, #T_57d1c_row2_col12, #T_57d1c_row2_col13, #T_57d1c_row3_col0, #T_57d1c_row3_col1, #T_57d1c_row3_col2, #T_57d1c_row3_col3, #T_57d1c_row3_col4, #T_57d1c_row3_col5, #T_57d1c_row3_col6, #T_57d1c_row3_col7, #T_57d1c_row3_col8, #T_57d1c_row3_col9, #T_57d1c_row3_col10, #T_57d1c_row3_col11, #T_57d1c_row3_col12, #T_57d1c_row3_col13, #T_57d1c_row4_col0, #T_57d1c_row4_col1, #T_57d1c_row4_col2, #T_57d1c_row4_col3, #T_57d1c_row4_col4, #T_57d1c_row4_col5, #T_57d1c_row4_col6, #T_57d1c_row4_col7, #T_57d1c_row4_col8, #T_57d1c_row4_col9, #T_57d1c_row4_col10, #T_57d1c_row4_col11, #T_57d1c_row4_col12, #T_57d1c_row4_col13 {\n",
              "  text-align: left;\n",
              "  white-space: pre-wrap;\n",
              "  word-wrap: break-word;\n",
              "  max-width: 400px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_57d1c\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_57d1c_level0_col0\" class=\"col_heading level0 col0\" >age</th>\n",
              "      <th id=\"T_57d1c_level0_col1\" class=\"col_heading level0 col1\" >gender</th>\n",
              "      <th id=\"T_57d1c_level0_col2\" class=\"col_heading level0 col2\" >political_party</th>\n",
              "      <th id=\"T_57d1c_level0_col3\" class=\"col_heading level0 col3\" >country</th>\n",
              "      <th id=\"T_57d1c_level0_col4\" class=\"col_heading level0 col4\" >language</th>\n",
              "      <th id=\"T_57d1c_level0_col5\" class=\"col_heading level0 col5\" >facebook_hours</th>\n",
              "      <th id=\"T_57d1c_level0_col6\" class=\"col_heading level0 col6\" >instagram_hours</th>\n",
              "      <th id=\"T_57d1c_level0_col7\" class=\"col_heading level0 col7\" >twitter_hours</th>\n",
              "      <th id=\"T_57d1c_level0_col8\" class=\"col_heading level0 col8\" >reddit_hours</th>\n",
              "      <th id=\"T_57d1c_level0_col9\" class=\"col_heading level0 col9\" >news_outlet</th>\n",
              "      <th id=\"T_57d1c_level0_col10\" class=\"col_heading level0 col10\" >headline</th>\n",
              "      <th id=\"T_57d1c_level0_col11\" class=\"col_heading level0 col11\" >example_bias_label</th>\n",
              "      <th id=\"T_57d1c_level0_col12\" class=\"col_heading level0 col12\" >pred_bias_label</th>\n",
              "      <th id=\"T_57d1c_level0_col13\" class=\"col_heading level0 col13\" >validate_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_57d1c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_57d1c_row0_col0\" class=\"data row0 col0\" >31</td>\n",
              "      <td id=\"T_57d1c_row0_col1\" class=\"data row0 col1\" >Male</td>\n",
              "      <td id=\"T_57d1c_row0_col2\" class=\"data row0 col2\" >Liberal</td>\n",
              "      <td id=\"T_57d1c_row0_col3\" class=\"data row0 col3\" >United States</td>\n",
              "      <td id=\"T_57d1c_row0_col4\" class=\"data row0 col4\" >English</td>\n",
              "      <td id=\"T_57d1c_row0_col5\" class=\"data row0 col5\" >10</td>\n",
              "      <td id=\"T_57d1c_row0_col6\" class=\"data row0 col6\" >20</td>\n",
              "      <td id=\"T_57d1c_row0_col7\" class=\"data row0 col7\" >12</td>\n",
              "      <td id=\"T_57d1c_row0_col8\" class=\"data row0 col8\" >15</td>\n",
              "      <td id=\"T_57d1c_row0_col9\" class=\"data row0 col9\" >CNN</td>\n",
              "      <td id=\"T_57d1c_row0_col10\" class=\"data row0 col10\" >The search for the 2022 Gerber Baby is on</td>\n",
              "      <td id=\"T_57d1c_row0_col11\" class=\"data row0 col11\" >is-biased</td>\n",
              "      <td id=\"T_57d1c_row0_col12\" class=\"data row0 col12\" >is-not-biased</td>\n",
              "      <td id=\"T_57d1c_row0_col13\" class=\"data row0 col13\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_57d1c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_57d1c_row1_col0\" class=\"data row1 col0\" >65</td>\n",
              "      <td id=\"T_57d1c_row1_col1\" class=\"data row1 col1\" >Female</td>\n",
              "      <td id=\"T_57d1c_row1_col2\" class=\"data row1 col2\" >Liberal</td>\n",
              "      <td id=\"T_57d1c_row1_col3\" class=\"data row1 col3\" >United States</td>\n",
              "      <td id=\"T_57d1c_row1_col4\" class=\"data row1 col4\" >English</td>\n",
              "      <td id=\"T_57d1c_row1_col5\" class=\"data row1 col5\" >3</td>\n",
              "      <td id=\"T_57d1c_row1_col6\" class=\"data row1 col6\" >2</td>\n",
              "      <td id=\"T_57d1c_row1_col7\" class=\"data row1 col7\" >0</td>\n",
              "      <td id=\"T_57d1c_row1_col8\" class=\"data row1 col8\" >0</td>\n",
              "      <td id=\"T_57d1c_row1_col9\" class=\"data row1 col9\" >FOX</td>\n",
              "      <td id=\"T_57d1c_row1_col10\" class=\"data row1 col10\" >Florida model Courtney Clenney spotted in Miami hotel with father after boyfriend's stabbing death</td>\n",
              "      <td id=\"T_57d1c_row1_col11\" class=\"data row1 col11\" >is-biased</td>\n",
              "      <td id=\"T_57d1c_row1_col12\" class=\"data row1 col12\" >is-biased</td>\n",
              "      <td id=\"T_57d1c_row1_col13\" class=\"data row1 col13\" >✔️ [True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_57d1c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_57d1c_row2_col0\" class=\"data row2 col0\" >48</td>\n",
              "      <td id=\"T_57d1c_row2_col1\" class=\"data row2 col1\" >Male</td>\n",
              "      <td id=\"T_57d1c_row2_col2\" class=\"data row2 col2\" >Independent</td>\n",
              "      <td id=\"T_57d1c_row2_col3\" class=\"data row2 col3\" >United States</td>\n",
              "      <td id=\"T_57d1c_row2_col4\" class=\"data row2 col4\" >English</td>\n",
              "      <td id=\"T_57d1c_row2_col5\" class=\"data row2 col5\" >1</td>\n",
              "      <td id=\"T_57d1c_row2_col6\" class=\"data row2 col6\" >2</td>\n",
              "      <td id=\"T_57d1c_row2_col7\" class=\"data row2 col7\" >1</td>\n",
              "      <td id=\"T_57d1c_row2_col8\" class=\"data row2 col8\" >1</td>\n",
              "      <td id=\"T_57d1c_row2_col9\" class=\"data row2 col9\" >BBC</td>\n",
              "      <td id=\"T_57d1c_row2_col10\" class=\"data row2 col10\" >Morad Tahbaz: British-US national detained in Iran on hunger strike</td>\n",
              "      <td id=\"T_57d1c_row2_col11\" class=\"data row2 col11\" >is-biased</td>\n",
              "      <td id=\"T_57d1c_row2_col12\" class=\"data row2 col12\" >is-not-biased</td>\n",
              "      <td id=\"T_57d1c_row2_col13\" class=\"data row2 col13\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_57d1c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_57d1c_row3_col0\" class=\"data row3 col0\" >23</td>\n",
              "      <td id=\"T_57d1c_row3_col1\" class=\"data row3 col1\" >Male</td>\n",
              "      <td id=\"T_57d1c_row3_col2\" class=\"data row3 col2\" >Conservative</td>\n",
              "      <td id=\"T_57d1c_row3_col3\" class=\"data row3 col3\" >United States</td>\n",
              "      <td id=\"T_57d1c_row3_col4\" class=\"data row3 col4\" >English</td>\n",
              "      <td id=\"T_57d1c_row3_col5\" class=\"data row3 col5\" >7</td>\n",
              "      <td id=\"T_57d1c_row3_col6\" class=\"data row3 col6\" >8</td>\n",
              "      <td id=\"T_57d1c_row3_col7\" class=\"data row3 col7\" >5</td>\n",
              "      <td id=\"T_57d1c_row3_col8\" class=\"data row3 col8\" >4</td>\n",
              "      <td id=\"T_57d1c_row3_col9\" class=\"data row3 col9\" >BBC</td>\n",
              "      <td id=\"T_57d1c_row3_col10\" class=\"data row3 col10\" >Who are the justices on the US Supreme Court?</td>\n",
              "      <td id=\"T_57d1c_row3_col11\" class=\"data row3 col11\" >is-biased</td>\n",
              "      <td id=\"T_57d1c_row3_col12\" class=\"data row3 col12\" >is-not-biased</td>\n",
              "      <td id=\"T_57d1c_row3_col13\" class=\"data row3 col13\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_57d1c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_57d1c_row4_col0\" class=\"data row4 col0\" >32</td>\n",
              "      <td id=\"T_57d1c_row4_col1\" class=\"data row4 col1\" >Male</td>\n",
              "      <td id=\"T_57d1c_row4_col2\" class=\"data row4 col2\" >Liberal</td>\n",
              "      <td id=\"T_57d1c_row4_col3\" class=\"data row4 col3\" >United States</td>\n",
              "      <td id=\"T_57d1c_row4_col4\" class=\"data row4 col4\" >English</td>\n",
              "      <td id=\"T_57d1c_row4_col5\" class=\"data row4 col5\" >1</td>\n",
              "      <td id=\"T_57d1c_row4_col6\" class=\"data row4 col6\" >1</td>\n",
              "      <td id=\"T_57d1c_row4_col7\" class=\"data row4 col7\" >1</td>\n",
              "      <td id=\"T_57d1c_row4_col8\" class=\"data row4 col8\" >1</td>\n",
              "      <td id=\"T_57d1c_row4_col9\" class=\"data row4 col9\" >CNN</td>\n",
              "      <td id=\"T_57d1c_row4_col10\" class=\"data row4 col10\" >Michelle Obama: Ketanji Brown Jackson gives Black women and girls ‘a new dream to dream’</td>\n",
              "      <td id=\"T_57d1c_row4_col11\" class=\"data row4 col11\" >is-biased</td>\n",
              "      <td id=\"T_57d1c_row4_col12\" class=\"data row4 col12\" >is-not-biased</td>\n",
              "      <td id=\"T_57d1c_row4_col13\" class=\"data row4 col13\" >False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7ef4fd136bc0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                <div style='\n",
              "                    text-align: center;\n",
              "                    font-size: 16px;\n",
              "                    font-weight: bold;\n",
              "                    color: #555;\n",
              "                    margin: 10px 0;'>\n",
              "                    ... 439 more rows not displayed ...\n",
              "                </div>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation score: 46.62\n"
          ]
        }
      ],
      "source": [
        "from dspy.evaluate import Evaluate\n",
        "# Set up the evaluator\n",
        "evaluate_on_devset = Evaluate(devset=devset, num_threads=4, display_progress=True, display_table=5)\n",
        "\n",
        "# Evaluate the optimized program\n",
        "eval_score = evaluate_on_devset(optimized_program, metric=validate_answer)\n",
        "\n",
        "# Print the evaluation score\n",
        "print(f\"Evaluation score: {eval_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lBbGHQ5PJp-q",
        "outputId": "7dbb3d03-f1d9-41a3-ced9-7f98e9e0c946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m\u001b[1mWARNING: Projected Language Model (LM) Calls\u001b[0m\n",
            "\n",
            "Please be advised that based on the parameters you have set, the maximum number of LM calls is projected as follows:\n",
            "\n",
            "\u001b[93m- Task Model: \u001b[94m\u001b[1m1776\u001b[0m\u001b[93m examples in dev set * \u001b[94m\u001b[1m15\u001b[0m\u001b[93m trials * \u001b[94m\u001b[1m# of LM calls in your program\u001b[0m\u001b[93m = (\u001b[94m\u001b[1m26640 * # of LM calls in your program\u001b[0m\u001b[93m) task model calls\u001b[0m\n",
            "\u001b[93m- Prompt Model: # data summarizer calls (max \u001b[94m\u001b[1m10\u001b[0m\u001b[93m) + \u001b[94m\u001b[1m8\u001b[0m\u001b[93m * \u001b[94m\u001b[1m1\u001b[0m\u001b[93m lm calls in program = \u001b[94m\u001b[1m18\u001b[0m\u001b[93m prompt model calls\u001b[0m\n",
            "\n",
            "\u001b[93m\u001b[1mEstimated Cost Calculation:\u001b[0m\n",
            "\n",
            "\u001b[93mTotal Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input Token + Avg Output Token Length per Call * Task Model Price per Output Token) \n",
            "            + (Number of calls to prompt model * (Avg Input Token Length per Call * Task Prompt Price per Input Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).\u001b[0m\n",
            "\n",
            "For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task\n",
            "and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may consider:\n",
            "\n",
            "\u001b[93m- Reducing the number of trials (`num_trials`), the size of the trainset, or the number of LM calls in your program.\u001b[0m\n",
            "\u001b[93m- Using a cheaper task model to optimize the prompt.\u001b[0m\n",
            "To proceed with the execution of this program, please confirm by typing \u001b[94m'y'\u001b[0m for yes or \u001b[94m'n'\u001b[0m for no.\n",
            "\n",
            "If you would like to bypass this confirmation step in future executions, set the \u001b[93m`requires_permission_to_run`\u001b[0m flag to \u001b[93m`False`.\u001b[0m\n",
            "\n",
            "\u001b[93mAwaiting your input...\u001b[0m\n",
            "\n",
            "Do you wish to continue? (y/n): y\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 25/1776 [00:05<06:54,  4.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 15 full traces after 26 examples in round 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 49/1776 [00:13<08:11,  3.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 15 full traces after 50 examples in round 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 31/1776 [00:07<06:43,  4.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 15 full traces after 32 examples in round 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 40/1776 [00:09<06:56,  4.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 15 full traces after 41 examples in round 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 45/1776 [00:11<07:06,  4.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 15 full traces after 46 examples in round 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 25/1776 [00:08<09:37,  3.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 15 full traces after 26 examples in round 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 35/1776 [00:09<07:59,  3.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 15 full traces after 36 examples in round 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-07-17 22:10:25,404] A new study created in memory with name: no-name-914758e9-2a13-4582-8ca1-c97b2396b5cb\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting trial #0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 19 / 39  (48.7):  39%|███▉      | 39/100 [00:03<00:05, 11.47it/s]INFO:backoff:Backing off request(...) for 0.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89895, Requested 1364. Please try again in 839ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 20 / 40  (50.0):  39%|███▉      | 39/100 [00:04<00:05, 11.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89987, Requested 1370. Please try again in 904ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89977, Requested 1372. Please try again in 899ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 22 / 42  (52.4):  42%|████▏     | 42/100 [00:06<00:26,  2.20it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89976, Requested 1372. Please try again in 898ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89976, Requested 1370. Please try again in 897ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 22 / 44  (50.0):  44%|████▍     | 44/100 [00:08<00:36,  1.54it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89985, Requested 1371. Please try again in 904ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 1.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89981, Requested 1370. Please try again in 900ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 23 / 45  (51.1):  45%|████▌     | 45/100 [00:09<00:39,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 1.3 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 23 / 46  (50.0):  46%|████▌     | 46/100 [00:10<00:42,  1.26it/s]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89984, Requested 1356. Please try again in 893ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 23 / 47  (48.9):  47%|████▋     | 47/100 [00:11<00:49,  1.08it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89975, Requested 1370. Please try again in 896ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 1.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89895, Requested 1356. Please try again in 834ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 24 / 49  (49.0):  49%|████▉     | 49/100 [00:13<00:44,  1.14it/s]INFO:backoff:Backing off request(...) for 9.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 90007, Requested 1370. Please try again in 918ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 9.7 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 25 / 50  (50.0):  50%|█████     | 50/100 [00:14<00:45,  1.09it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89963, Requested 1369. Please try again in 888ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 25 / 52  (48.1):  52%|█████▏    | 52/100 [00:16<00:45,  1.05it/s]INFO:backoff:Backing off request(...) for 1.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89978, Requested 1369. Please try again in 898ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 28 / 55  (50.9):  55%|█████▌    | 55/100 [00:18<00:41,  1.07it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89981, Requested 1369. Please try again in 900ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 29 / 57  (50.9):  57%|█████▋    | 57/100 [00:20<00:41,  1.04it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89985, Requested 1367. Please try again in 901ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 30 / 59  (50.8):  59%|█████▉    | 59/100 [00:22<00:39,  1.03it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89977, Requested 1354. Please try again in 887ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 32 / 61  (52.5):  61%|██████    | 61/100 [00:24<00:38,  1.02it/s]INFO:backoff:Backing off request(...) for 17.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89970, Requested 1370. Please try again in 893ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89936, Requested 1364. Please try again in 866ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 17.7 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 32 / 62  (51.6):  62%|██████▏   | 62/100 [00:25<00:36,  1.05it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88683, Requested 1354. Please try again in 24ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 33 / 64  (51.6):  64%|██████▍   | 64/100 [00:27<00:34,  1.05it/s]INFO:backoff:Backing off request(...) for 1.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89976, Requested 1354. Please try again in 886ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 35 / 67  (52.2):  67%|██████▋   | 67/100 [00:30<00:32,  1.03it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89966, Requested 1370. Please try again in 890ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 35 / 69  (50.7):  69%|██████▉   | 69/100 [00:32<00:29,  1.04it/s]INFO:backoff:Backing off request(...) for 1.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89979, Requested 1370. Please try again in 899ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 38 / 72  (52.8):  72%|███████▏  | 72/100 [00:35<00:26,  1.04it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89968, Requested 1370. Please try again in 892ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 39 / 74  (52.7):  74%|███████▍  | 74/100 [00:37<00:24,  1.05it/s]INFO:backoff:Backing off request(...) for 1.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89970, Requested 1370. Please try again in 893ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.8 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 41 / 77  (53.2):  77%|███████▋  | 77/100 [00:40<00:21,  1.05it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89969, Requested 1362. Please try again in 887ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 42 / 79  (53.2):  79%|███████▉  | 79/100 [00:42<00:20,  1.02it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89939, Requested 1362. Please try again in 867ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 43 / 80  (53.8):  80%|████████  | 80/100 [00:43<00:18,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88642, Requested 1368. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 44 / 81  (54.3):  81%|████████  | 81/100 [00:44<00:18,  1.02it/s]INFO:backoff:Backing off request(...) for 6.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89988, Requested 1370. Please try again in 905ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 6.5 seconds after 7 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 45 / 82  (54.9):  82%|████████▏ | 82/100 [00:45<00:17,  1.01it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89980, Requested 1371. Please try again in 900ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 46 / 83  (55.4):  83%|████████▎ | 83/100 [00:45<00:16,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 46 / 84  (54.8):  84%|████████▍ | 84/100 [00:46<00:15,  1.05it/s]INFO:backoff:Backing off request(...) for 1.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89984, Requested 1371. Please try again in 903ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 49 / 87  (56.3):  87%|████████▋ | 87/100 [00:49<00:12,  1.05it/s]INFO:backoff:Backing off request(...) for 1.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89960, Requested 1371. Please try again in 887ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.5 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 51 / 89  (57.3):  89%|████████▉ | 89/100 [00:51<00:10,  1.02it/s]INFO:backoff:Backing off request(...) for 47.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89981, Requested 1370. Please try again in 900ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 47.4 seconds after 8 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 51 / 90  (56.7):  90%|█████████ | 90/100 [00:52<00:09,  1.02it/s]INFO:backoff:Backing off request(...) for 1.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88631, Requested 1371. Please try again in 1ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.7 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 53 / 93  (57.0):  93%|█████████▎| 93/100 [00:55<00:06,  1.04it/s]INFO:backoff:Backing off request(...) for 4.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89981, Requested 1371. Please try again in 901ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 54 / 94  (57.4):  94%|█████████▍| 94/100 [00:56<00:05,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 4.3 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88642, Requested 1365. Please try again in 4ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 55 / 100  (55.0): 100%|██████████| 100/100 [01:40<00:00,  1.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 55 / 100  (55.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 17 / 43  (39.5):  42%|████▏     | 42/100 [00:03<00:05, 10.69it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89895, Requested 1365. Please try again in 840ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89889, Requested 1361. Please try again in 833ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 17 / 45  (37.8):  45%|████▌     | 45/100 [00:05<00:18,  3.05it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89965, Requested 1361. Please try again in 884ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 1.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89986, Requested 1365. Please try again in 900ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 17 / 47  (36.2):  47%|████▋     | 47/100 [00:07<00:30,  1.76it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89981, Requested 1361. Please try again in 894ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 18 / 48  (37.5):  48%|████▊     | 48/100 [00:08<00:34,  1.49it/s]INFO:backoff:Backing off request(...) for 1.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89970, Requested 1365. Please try again in 890ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.5 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 18 / 49  (36.7):  49%|████▉     | 49/100 [00:09<00:36,  1.38it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89982, Requested 1356. Please try again in 892ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 18 / 50  (36.0):  50%|█████     | 50/100 [00:10<00:38,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 18 / 51  (35.3):  51%|█████     | 51/100 [00:11<00:40,  1.20it/s]INFO:backoff:Backing off request(...) for 5.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89979, Requested 1365. Please try again in 896ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89976, Requested 1364. Please try again in 893ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 18 / 52  (34.6):  52%|█████▏    | 52/100 [00:12<00:41,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 5.1 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 19 / 54  (35.2):  54%|█████▍    | 54/100 [00:14<00:43,  1.07it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89979, Requested 1363. Please try again in 894ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 20 / 57  (35.1):  57%|█████▋    | 57/100 [00:17<00:41,  1.04it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 21 / 58  (36.2):  58%|█████▊    | 58/100 [00:18<00:42,  1.00s/it]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 6.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89767, Requested 1365. Please try again in 754ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 21 / 59  (35.6):  59%|█████▉    | 59/100 [00:20<00:49,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 6.3 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 22 / 60  (36.7):  60%|██████    | 60/100 [00:20<00:36,  1.11it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89920, Requested 1364. Please try again in 856ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 22 / 61  (36.1):  61%|██████    | 61/100 [00:21<00:33,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 23 / 62  (37.1):  62%|██████▏   | 62/100 [00:21<00:34,  1.11it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 2.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.0 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "INFO:backoff:Backing off request(...) for 1.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.6 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 1.7 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 12.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 12.2 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 3.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.9 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 2.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.9 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 5.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 5.5 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 3.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.5 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 3.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.6 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 2.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.9 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 9.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 9.7 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 9.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 9.8 seconds after 7 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 8.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 8.6 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 17.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 17.0 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 37 / 83  (44.6):  82%|████████▏ | 82/100 [00:56<00:04,  4.49it/s]INFO:backoff:Backing off request(...) for 2.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 37 / 85  (43.5):  84%|████████▍ | 84/100 [00:57<00:03,  5.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.5 seconds after 8 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 40 / 100  (40.0): 100%|██████████| 100/100 [01:04<00:00,  1.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 40 / 100  (40.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 2 / 8  (25.0):   8%|▊         | 8/100 [00:02<00:26,  3.47it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 2 / 9  (22.2):   9%|▉         | 9/100 [00:02<00:39,  2.31it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 3 / 11  (27.3):  11%|█         | 11/100 [00:04<01:01,  1.45it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88897, Requested 1364. Please try again in 174ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 4 / 12  (33.3):  12%|█▏        | 12/100 [00:05<01:10,  1.26it/s]INFO:backoff:Backing off request(...) for 1.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89227, Requested 1364. Please try again in 394ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 4 / 13  (30.8):  13%|█▎        | 13/100 [00:06<01:13,  1.19it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 4 / 14  (28.6):  14%|█▍        | 14/100 [00:07<01:14,  1.15it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88643, Requested 1359. Please try again in 1ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 5 / 16  (31.2):  16%|█▌        | 16/100 [00:09<01:18,  1.08it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88902, Requested 1358. Please try again in 173ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 6 / 17  (35.3):  17%|█▋        | 17/100 [00:10<01:15,  1.11it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 6 / 18  (33.3):  18%|█▊        | 18/100 [00:11<01:19,  1.03it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89102, Requested 1357. Please try again in 306ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 7 / 20  (35.0):  20%|██        | 20/100 [00:13<01:15,  1.06it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 8 / 21  (38.1):  21%|██        | 21/100 [00:14<01:15,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 8 / 22  (36.4):  22%|██▏       | 22/100 [00:15<01:14,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 8 / 23  (34.8):  23%|██▎       | 23/100 [00:16<01:13,  1.05it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89871, Requested 1360. Please try again in 820ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 8 / 24  (33.3):  24%|██▍       | 24/100 [00:17<01:14,  1.02it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88813, Requested 1356. Please try again in 112ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 8 / 25  (32.0):  25%|██▌       | 25/100 [00:18<01:14,  1.00it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89854, Requested 1371. Please try again in 816ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 9 / 27  (33.3):  27%|██▋       | 27/100 [00:20<01:09,  1.05it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "INFO:backoff:Backing off request(...) for 1.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 10 / 28  (35.7):  28%|██▊       | 28/100 [00:21<01:10,  1.02it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89792, Requested 1366. Please try again in 772ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 12 / 30  (40.0):  30%|███       | 30/100 [00:23<01:05,  1.06it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 12 / 31  (38.7):  31%|███       | 31/100 [00:24<01:06,  1.03it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 13 / 32  (40.6):  32%|███▏      | 32/100 [00:25<01:06,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 14 / 33  (42.4):  33%|███▎      | 33/100 [00:26<01:02,  1.07it/s]INFO:backoff:Backing off request(...) for 5.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88659, Requested 1360. Please try again in 12ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 5.8 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 14 / 34  (41.2):  34%|███▍      | 34/100 [00:27<01:04,  1.02it/s]INFO:backoff:Backing off request(...) for 2.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88698, Requested 1369. Please try again in 44ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.0 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 15 / 36  (41.7):  36%|███▌      | 36/100 [00:29<01:01,  1.04it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 15 / 37  (40.5):  36%|███▌      | 36/100 [00:30<01:01,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 15 / 38  (39.5):  38%|███▊      | 38/100 [00:30<00:59,  1.05it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88652, Requested 1364. Please try again in 10ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 15 / 39  (38.5):  39%|███▉      | 39/100 [00:31<00:58,  1.04it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88975, Requested 1368. Please try again in 228ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 15 / 42  (35.7):  42%|████▏     | 42/100 [00:34<00:57,  1.01it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 13.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88988, Requested 1360. Please try again in 232ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 13.8 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 16 / 44  (36.4):  44%|████▍     | 44/100 [00:36<00:53,  1.04it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88741, Requested 1371. Please try again in 74ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 17 / 46  (37.0):  46%|████▌     | 46/100 [00:38<00:51,  1.04it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88969, Requested 1371. Please try again in 226ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 18 / 47  (38.3):  47%|████▋     | 47/100 [00:39<00:52,  1.00it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89071, Requested 1360. Please try again in 287ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 20 / 49  (40.8):  49%|████▉     | 49/100 [00:41<00:48,  1.05it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 21 / 50  (42.0):  50%|█████     | 50/100 [00:42<00:49,  1.01it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88678, Requested 1368. Please try again in 30ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 21 / 52  (40.4):  52%|█████▏    | 52/100 [00:44<00:45,  1.05it/s]INFO:backoff:Backing off request(...) for 3.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 22 / 53  (41.5):  52%|█████▏    | 52/100 [00:45<00:45,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.9 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 25 / 57  (43.9):  57%|█████▋    | 57/100 [00:49<00:41,  1.04it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88915, Requested 1367. Please try again in 188ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 27 / 59  (45.8):  59%|█████▉    | 59/100 [00:51<00:39,  1.03it/s]INFO:backoff:Backing off request(...) for 5.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89053, Requested 1360. Please try again in 275ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 5.0 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 2.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 27 / 60  (45.0):  60%|██████    | 60/100 [00:52<00:38,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.4 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88870, Requested 1367. Please try again in 158ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 30 / 64  (46.9):  64%|██████▍   | 64/100 [00:56<00:33,  1.07it/s]INFO:backoff:Backing off request(...) for 3.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.0 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 31 / 65  (47.7):  65%|██████▌   | 65/100 [00:57<00:33,  1.04it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88735, Requested 1368. Please try again in 68ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 32 / 66  (48.5):  66%|██████▌   | 66/100 [00:58<00:32,  1.05it/s]INFO:backoff:Backing off request(...) for 54.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 32 / 67  (47.8):  67%|██████▋   | 67/100 [00:59<00:32,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 54.6 seconds after 7 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 33 / 68  (48.5):  68%|██████▊   | 68/100 [01:00<00:31,  1.01it/s]INFO:backoff:Backing off request(...) for 1.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88706, Requested 1368. Please try again in 49ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 34 / 71  (47.9):  71%|███████   | 71/100 [01:02<00:27,  1.07it/s]INFO:backoff:Backing off request(...) for 4.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 4.8 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 35 / 72  (48.6):  72%|███████▏  | 72/100 [01:03<00:27,  1.03it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 37 / 75  (49.3):  75%|███████▌  | 75/100 [01:06<00:23,  1.05it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 40 / 78  (51.3):  78%|███████▊  | 78/100 [01:09<00:21,  1.02it/s]INFO:backoff:Backing off request(...) for 6.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88686, Requested 1367. Please try again in 35ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 6.7 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 41 / 79  (51.9):  79%|███████▉  | 79/100 [01:10<00:20,  1.04it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88775, Requested 1368. Please try again in 95ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 42 / 82  (51.2):  82%|████████▏ | 82/100 [01:13<00:17,  1.05it/s]INFO:backoff:Backing off request(...) for 1.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88731, Requested 1368. Please try again in 66ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 43 / 86  (50.0):  86%|████████▌ | 86/100 [01:17<00:13,  1.02it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 43 / 87  (49.4):  87%|████████▋ | 87/100 [01:18<00:12,  1.02it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88874, Requested 1371. Please try again in 163ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 45 / 90  (50.0):  90%|█████████ | 90/100 [01:21<00:09,  1.06it/s]INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 46 / 91  (50.5):  91%|█████████ | 91/100 [01:22<00:08,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 47 / 93  (50.5):  93%|█████████▎| 93/100 [01:24<00:06,  1.05it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89094, Requested 1364. Please try again in 305ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 48 / 94  (51.1):  94%|█████████▍| 94/100 [01:25<00:05,  1.03it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 48 / 95  (50.5):  95%|█████████▌| 95/100 [01:26<00:04,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 49 / 96  (51.0):  96%|█████████▌| 96/100 [01:27<00:03,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.8 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 51 / 100  (51.0): 100%|██████████| 100/100 [01:53<00:00,  1.14s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 51 / 100  (51.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 10 / 15  (66.7):  14%|█▍        | 14/100 [00:02<00:15,  5.41it/s]INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 11 / 17  (64.7):  16%|█▌        | 16/100 [00:02<00:15,  5.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 19 / 30  (63.3):  30%|███       | 30/100 [00:06<00:19,  3.53it/s]INFO:backoff:Backing off request(...) for 1.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89107, Requested 1358. Please try again in 310ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88971, Requested 1358. Please try again in 219ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 19 / 32  (59.4):  32%|███▏      | 32/100 [00:07<00:37,  1.82it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89985, Requested 1358. Please try again in 895ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 20 / 33  (60.6):  33%|███▎      | 33/100 [00:08<00:45,  1.48it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88636, Requested 1366. Please try again in 1ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 21 / 34  (61.8):  34%|███▍      | 34/100 [00:09<00:48,  1.36it/s]INFO:backoff:Backing off request(...) for 2.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88741, Requested 1358. Please try again in 66ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.9 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 21 / 35  (60.0):  35%|███▌      | 35/100 [00:10<00:54,  1.20it/s]INFO:backoff:Backing off request(...) for 1.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88771, Requested 1358. Please try again in 86ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.6 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 21 / 39  (53.8):  39%|███▉      | 39/100 [00:14<00:55,  1.11it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88802, Requested 1365. Please try again in 111ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 7.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 22 / 40  (55.0):  40%|████      | 40/100 [00:15<00:55,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 7.2 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 22 / 42  (52.4):  42%|████▏     | 42/100 [00:17<00:54,  1.07it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88962, Requested 1365. Please try again in 218ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 23 / 44  (52.3):  44%|████▍     | 44/100 [00:19<00:54,  1.04it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Requested 1367. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 24 / 47  (51.1):  47%|████▋     | 47/100 [00:22<00:50,  1.04it/s]INFO:backoff:Backing off request(...) for 1.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89697, Requested 1367. Please try again in 709ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 24 / 48  (50.0):  48%|████▊     | 48/100 [00:23<00:50,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 25 / 49  (51.0):  49%|████▉     | 49/100 [00:24<00:50,  1.02it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88922, Requested 1371. Please try again in 195ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 26 / 51  (51.0):  51%|█████     | 51/100 [00:26<00:46,  1.06it/s]INFO:backoff:Backing off request(...) for 1.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88858, Requested 1367. Please try again in 150ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.7 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 26 / 52  (50.0):  52%|█████▏    | 52/100 [00:27<00:47,  1.02it/s]INFO:backoff:Backing off request(...) for 0.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88753, Requested 1363. Please try again in 77ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89983, Requested 1366. Please try again in 899ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 28 / 54  (51.9):  54%|█████▍    | 54/100 [00:29<00:45,  1.02it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89010, Requested 1370. Please try again in 253ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 28 / 55  (50.9):  55%|█████▌    | 55/100 [00:30<00:43,  1.04it/s]INFO:backoff:Backing off request(...) for 6.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88991, Requested 1367. Please try again in 238ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 6.7 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 29 / 57  (50.9):  57%|█████▋    | 57/100 [00:32<00:41,  1.03it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88754, Requested 1370. Please try again in 82ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 29 / 60  (48.3):  60%|██████    | 60/100 [00:34<00:37,  1.06it/s]INFO:backoff:Backing off request(...) for 1.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89788, Requested 1370. Please try again in 772ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 30 / 61  (49.2):  61%|██████    | 61/100 [00:36<00:38,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.2 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 31 / 64  (48.4):  64%|██████▍   | 64/100 [00:38<00:34,  1.05it/s]INFO:backoff:Backing off request(...) for 6.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88998, Requested 1370. Please try again in 245ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 14.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88891, Requested 1367. Please try again in 172ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 6.9 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 14.4 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 32 / 67  (47.8):  67%|██████▋   | 67/100 [00:41<00:31,  1.05it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88644, Requested 1362. Please try again in 4ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 35 / 72  (48.6):  72%|███████▏  | 72/100 [00:46<00:26,  1.05it/s]INFO:backoff:Backing off request(...) for 4.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89973, Requested 1370. Please try again in 895ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 4.0 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 37 / 77  (48.1):  77%|███████▋  | 77/100 [00:51<00:24,  1.07s/it]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89966, Requested 1369. Please try again in 890ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 37 / 78  (47.4):  78%|███████▊  | 78/100 [00:52<00:20,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88633, Requested 1370. Please try again in 2ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 39 / 80  (48.8):  80%|████████  | 80/100 [00:54<00:18,  1.07it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89974, Requested 1366. Please try again in 893ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 13.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89953, Requested 1367. Please try again in 880ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 39 / 81  (48.1):  81%|████████  | 81/100 [00:55<00:17,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 13.0 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89957, Requested 1370. Please try again in 884ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 41 / 83  (49.4):  83%|████████▎ | 83/100 [00:57<00:16,  1.05it/s]INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89975, Requested 1370. Please try again in 896ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 44 / 87  (50.6):  86%|████████▌ | 86/100 [00:59<00:13,  1.03it/s]INFO:backoff:Backing off request(...) for 3.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89983, Requested 1370. Please try again in 902ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 45 / 88  (51.1):  88%|████████▊ | 88/100 [01:00<00:07,  1.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.7 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88638, Requested 1365. Please try again in 2ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 49 / 92  (53.3):  92%|█████████▏| 92/100 [01:04<00:06,  1.20it/s]INFO:backoff:Backing off request(...) for 7.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89982, Requested 1370. Please try again in 901ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Average Metric: 50 / 93  (53.8):  92%|█████████▏| 92/100 [01:05<00:06,  1.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 7.8 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 53 / 97  (54.6):  97%|█████████▋| 97/100 [01:08<00:02,  1.07it/s]INFO:backoff:Backing off request(...) for 51.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89983, Requested 1367. Please try again in 900ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 51.9 seconds after 7 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 54 / 99  (54.5):  99%|█████████▉| 99/100 [01:12<00:01,  1.56s/it]INFO:backoff:Backing off request(...) for 94.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 94.3 seconds after 8 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 55 / 100  (55.0): 100%|██████████| 100/100 [03:39<00:00,  2.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 55 / 100  (55.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 2 / 8  (25.0):   8%|▊         | 8/100 [00:02<00:29,  3.16it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 2 / 9  (22.2):   9%|▉         | 9/100 [00:02<00:30,  3.03it/s]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 3 / 11  (27.3):  11%|█         | 11/100 [00:03<00:27,  3.28it/s]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 4 / 12  (33.3):  12%|█▏        | 12/100 [00:03<00:25,  3.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 5 / 18  (27.8):  18%|█▊        | 18/100 [00:05<00:25,  3.20it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 7 / 21  (33.3):  21%|██        | 21/100 [00:06<00:23,  3.38it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 8 / 22  (36.4):  22%|██▏       | 22/100 [00:06<00:22,  3.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 9 / 23  (39.1):  23%|██▎       | 23/100 [00:06<00:21,  3.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 13 / 29  (44.8):  29%|██▉       | 29/100 [00:08<00:19,  3.69it/s]INFO:backoff:Backing off request(...) for 3.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 13 / 30  (43.3):  30%|███       | 30/100 [00:08<00:20,  3.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.9 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 13 / 32  (40.6):  32%|███▏      | 32/100 [00:09<00:21,  3.17it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 14 / 33  (42.4):  33%|███▎      | 33/100 [00:09<00:21,  3.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 18 / 44  (40.9):  44%|████▍     | 44/100 [00:12<00:16,  3.39it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 20 / 47  (42.6):  47%|████▋     | 47/100 [00:13<00:15,  3.34it/s]INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 21 / 52  (40.4):  52%|█████▏    | 52/100 [00:15<00:15,  3.11it/s]INFO:backoff:Backing off request(...) for 4.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 4.1 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 23 / 55  (41.8):  55%|█████▌    | 55/100 [00:16<00:14,  3.00it/s]INFO:backoff:Backing off request(...) for 2.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.3 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 23 / 57  (40.4):  57%|█████▋    | 57/100 [00:17<00:14,  3.00it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 24 / 58  (41.4):  58%|█████▊    | 58/100 [00:17<00:14,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 29 / 67  (43.3):  67%|██████▋   | 67/100 [00:20<00:10,  3.27it/s]INFO:backoff:Backing off request(...) for 3.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 29 / 68  (42.6):  68%|██████▊   | 68/100 [00:20<00:10,  2.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 3.0 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 30 / 70  (42.9):  70%|███████   | 70/100 [00:21<00:11,  2.72it/s]INFO:backoff:Backing off request(...) for 2.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.9 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 32 / 72  (44.4):  72%|███████▏  | 72/100 [00:22<00:09,  2.89it/s]INFO:backoff:Backing off request(...) for 1.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 33 / 73  (45.2):  73%|███████▎  | 73/100 [00:22<00:08,  3.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.1 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 37 / 85  (43.5):  85%|████████▌ | 85/100 [00:26<00:04,  3.45it/s]INFO:backoff:Backing off request(...) for 2.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "INFO:backoff:Backing off request(...) for 13.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.5 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 13.1 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 40 / 88  (45.5):  88%|████████▊ | 88/100 [00:27<00:04,  2.84it/s]INFO:backoff:Backing off request(...) for 6.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 41 / 89  (46.1):  89%|████████▉ | 89/100 [00:27<00:03,  3.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 6.4 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 43 / 92  (46.7):  92%|█████████▏| 92/100 [00:30<00:05,  1.43it/s]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 44 / 93  (47.3):  92%|█████████▏| 92/100 [00:30<00:05,  1.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 45 / 95  (47.4):  95%|█████████▌| 95/100 [00:32<00:04,  1.16it/s]INFO:backoff:Backing off request(...) for 1.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88754, Requested 1361. Please try again in 76ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 48 / 100  (48.0): 100%|██████████| 100/100 [00:39<00:00,  2.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 48 / 100  (48.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 2 / 3  (66.7):   3%|▎         | 3/100 [00:00<00:25,  3.76it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89904, Requested 1360. Please try again in 842ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 3 / 5  (60.0):   5%|▌         | 5/100 [00:02<01:02,  1.53it/s]INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89274, Requested 1360. Please try again in 422ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 4 / 6  (66.7):   6%|▌         | 6/100 [00:03<01:11,  1.31it/s]INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88828, Requested 1359. Please try again in 124ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 4 / 7  (57.1):   7%|▋         | 7/100 [00:04<01:16,  1.21it/s]INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88670, Requested 1369. Please try again in 26ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 5 / 8  (62.5):   8%|▊         | 8/100 [00:05<01:19,  1.16it/s]INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88661, Requested 1368. Please try again in 19ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 6 / 10  (60.0):  10%|█         | 10/100 [00:07<01:25,  1.05it/s]INFO:backoff:Backing off request(...) for 1.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88720, Requested 1369. Please try again in 59ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 7 / 11  (63.6):  11%|█         | 11/100 [00:08<01:21,  1.09it/s]INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 88886, Requested 1368. Please try again in 169ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 7 / 12  (58.3):  12%|█▏        | 12/100 [00:09<01:22,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 8 / 14  (57.1):  14%|█▍        | 14/100 [00:11<01:30,  1.06s/it]INFO:backoff:Backing off request(...) for 2.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "Average Metric: 9 / 15  (60.0):  15%|█▌        | 15/100 [00:12<01:19,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.5 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-instruct in organization org-Pials8RU5D0s6VJkSvWbY8Wi on tokens per min (TPM): Limit 90000, Used 89238, Requested 1369. Please try again in 404ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.8 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 0.4 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "INFO:backoff:Backing off request(...) for 6.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 6.4 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 14.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "INFO:backoff:Backing off request(...) for 2.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 14.3 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 2.5 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 2.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.9 seconds after 3 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 2.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.1 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 2.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
            "INFO:backoff:Backing off request(...) for 10.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.0 seconds after 4 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n",
            "Backing off 10.2 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 1.2 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 4.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 4.3 seconds after 5 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 4.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 4.4 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 2.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 2.1 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 13.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 13.3 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 18.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 18.7 seconds after 6 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 54.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 54.8 seconds after 7 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 49.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 49.0 seconds after 7 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 22.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 22.5 seconds after 7 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 61.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 61.0 seconds after 7 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 69.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 69.9 seconds after 8 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 28.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 28.1 seconds after 8 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 43.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 43.0 seconds after 8 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 32.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 32.7 seconds after 9 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 79.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 79.6 seconds after 8 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 144.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 144.5 seconds after 9 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 34.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 34.7 seconds after 9 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 194.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 194.1 seconds after 10 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:backoff:Backing off request(...) for 389.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backing off 389.7 seconds after 10 tries calling function <function GPT3.request at 0x7ef5358d0c10> with kwargs {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W 2024-07-17 22:22:42,695] Trial 0 failed with parameters: {'139590597867888_predictor_instruction': 1, '139590597867888_predictor_demos': 3} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dspy/evaluate/evaluate.py\", line 81, in _execute_multi_thread\n",
            "    for future in as_completed(futures):\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 245, in as_completed\n",
            "    waiter.event.wait(wait_timeout)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 607, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 320, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dspy/teleprompt/mipro_optimizer.py\", line 432, in objective\n",
            "    split_score = evaluate(candidate_program, devset=split_trainset, display_table=0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dspy/evaluate/evaluate.py\", line 163, in __call__\n",
            "    reordered_devset, ncorrect, ntotal = self._execute_multi_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dspy/evaluate/evaluate.py\", line 75, in _execute_multi_thread\n",
            "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 649, in __exit__\n",
            "    self.shutdown(wait=True)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 235, in shutdown\n",
            "    t.join()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "[W 2024-07-17 22:22:42,706] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/evaluate/evaluate.py\u001b[0m in \u001b[0;36m_execute_multi_thread\u001b[0;34m(self, wrapped_program, devset, num_threads, display_progress)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mexample_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2c352918bd5f>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Compile the module with the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m compiled_program = teleprompter.compile(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mSubjectiveBiasLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrainset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/teleprompt/mipro_optimizer.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, student, trainset, num_trials, max_bootstrapped_demos, max_labeled_demos, eval_kwargs, seed, view_data, view_examples, requires_permission_to_run)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_program\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_stats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     ):\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/teleprompt/mipro_optimizer.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    430\u001b[0m                         \u001b[0mend_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                         \u001b[0msplit_trainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                         \u001b[0msplit_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_program\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_trainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_table\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{i}st split score: {split_score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/evaluate/evaluate.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, display, return_all_scores, return_outputs)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 wrapped_program, devset, display_progress)\n\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             reordered_devset, ncorrect, ntotal = self._execute_multi_thread(\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0mwrapped_program\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mdevset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/evaluate/evaluate.py\u001b[0m in \u001b[0;36m_execute_multi_thread\u001b[0;34m(self, wrapped_program, devset, num_threads, display_progress)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mreordered_devset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_threads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             futures = {executor.submit(wrapped_program, idx, arg)\n\u001b[1;32m     77\u001b[0m                        for idx, arg in devset}\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from dspy.teleprompt import MIPRO\n",
        "# Configure the MIPRO optimizer\n",
        "teleprompter = MIPRO(\n",
        "    prompt_model=turbo,\n",
        "    task_model=turbo,\n",
        "    metric=validate_answer,\n",
        "    num_candidates=8,  # Adjust based on your requirements\n",
        "    init_temperature=1.0\n",
        ")\n",
        "\n",
        "# Additional kwargs for evaluation\n",
        "kwargs = dict(num_threads=4, display_progress=True, display_table=5)\n",
        "\n",
        "# Compile the module with the training data\n",
        "compiled_program = teleprompter.compile(\n",
        "    SubjectiveBiasLabel(),\n",
        "    trainset=trainset,\n",
        "    num_trials=15,  # Adjust based on your computational resources\n",
        "    max_bootstrapped_demos=15,  # Adjust to manage token limits and computational load\n",
        "    max_labeled_demos=4,  # Adjust to manage token limits and computational load\n",
        "    eval_kwargs=kwargs\n",
        ")\n",
        "\n",
        "# Evaluate the compiled program\n",
        "eval_score = Evaluate(compiled_program, metric=validate_answer, devset=devset, **kwargs)\n",
        "\n",
        "# Print the evaluation score\n",
        "print(f\"Evaluation score: {eval_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njyulqVKipoo"
      },
      "source": [
        "## END OF TRIAL TUTORIAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib58Boj9A44p"
      },
      "source": [
        "# MIPRO Optimizer (Bayesian)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZmYepGlN5JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7PA2LDLB9_A",
        "outputId": "f7871f7f-6400-44ab-b2e0-ecfd33668c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2220, 24) \n",
            " (1000, 24)\n"
          ]
        }
      ],
      "source": [
        "# TRAINING SIZE\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "dataframe = pd.read_csv('cleaned_data_processed.csv')\n",
        "\n",
        "n=1000\n",
        "random_seed = 69\n",
        "df = dataframe.sample(n=n, random_state=random_seed)\n",
        "print(dataframe.shape, '\\n', df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbQsH005-5WD",
        "outputId": "5109693e-cf79-48f2-fd4b-afdccd87fefc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "480 120\n",
            "still need to consider two datasets segmented on the political party, training two models on the respective political party and then using those model predictions.\n",
            "\u001b[93m\u001b[1mWARNING: Projected Language Model (LM) Calls\u001b[0m\n",
            "\n",
            "Please be advised that based on the parameters you have set, the maximum number of LM calls is projected as follows:\n",
            "\n",
            "\u001b[93m- Task Model: \u001b[94m\u001b[1m480\u001b[0m\u001b[93m examples in dev set * \u001b[94m\u001b[1m1\u001b[0m\u001b[93m trials * \u001b[94m\u001b[1m# of LM calls in your program\u001b[0m\u001b[93m = (\u001b[94m\u001b[1m480 * # of LM calls in your program\u001b[0m\u001b[93m) task model calls\u001b[0m\n",
            "\u001b[93m- Prompt Model: # data summarizer calls (max \u001b[94m\u001b[1m10\u001b[0m\u001b[93m) + \u001b[94m\u001b[1m25\u001b[0m\u001b[93m * \u001b[94m\u001b[1m5\u001b[0m\u001b[93m lm calls in program = \u001b[94m\u001b[1m135\u001b[0m\u001b[93m prompt model calls\u001b[0m\n",
            "\n",
            "\u001b[93m\u001b[1mEstimated Cost Calculation:\u001b[0m\n",
            "\n",
            "\u001b[93mTotal Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input Token + Avg Output Token Length per Call * Task Model Price per Output Token) \n",
            "            + (Number of calls to prompt model * (Avg Input Token Length per Call * Task Prompt Price per Input Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).\u001b[0m\n",
            "\n",
            "For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task\n",
            "and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may consider:\n",
            "\n",
            "\u001b[93m- Reducing the number of trials (`num_trials`), the size of the trainset, or the number of LM calls in your program.\u001b[0m\n",
            "\u001b[93m- Using a cheaper task model to optimize the prompt.\u001b[0m\n",
            "To proceed with the execution of this program, please confirm by typing \u001b[94m'y'\u001b[0m for yes or \u001b[94m'n'\u001b[0m for no.\n",
            "\n",
            "If you would like to bypass this confirmation step in future executions, set the \u001b[93m`requires_permission_to_run`\u001b[0m flag to \u001b[93m`False`.\u001b[0m\n",
            "\n",
            "\u001b[93mAwaiting your input...\u001b[0m\n",
            "\n",
            "Do you wish to continue? (y/n): y\n",
            "Creating basic bootstrap: 1/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 13/480 [00:30<18:07,  2.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 14 examples in round 0.\n",
            "Creating basic bootstrap: 2/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 13/480 [00:29<17:43,  2.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 14 examples in round 0.\n",
            "Creating basic bootstrap: 3/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 10/480 [00:23<18:06,  2.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 11 examples in round 0.\n",
            "Creating basic bootstrap: 4/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 17/480 [00:25<11:42,  1.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 18 examples in round 0.\n",
            "Creating basic bootstrap: 5/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 20/480 [00:49<19:01,  2.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 21 examples in round 0.\n",
            "Creating basic bootstrap: 6/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 15/480 [00:27<14:01,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 16 examples in round 0.\n",
            "Creating basic bootstrap: 7/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 13/480 [00:20<12:15,  1.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 14 examples in round 0.\n",
            "Creating basic bootstrap: 8/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 17/480 [00:33<15:16,  1.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 18 examples in round 0.\n",
            "Creating basic bootstrap: 9/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 18/480 [00:32<13:59,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 19 examples in round 0.\n",
            "Creating basic bootstrap: 10/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 17/480 [00:32<14:44,  1.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 18 examples in round 0.\n",
            "Creating basic bootstrap: 11/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 24/480 [00:43<13:53,  1.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 25 examples in round 0.\n",
            "Creating basic bootstrap: 12/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 18/480 [00:33<14:13,  1.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 19 examples in round 0.\n",
            "Creating basic bootstrap: 13/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 27/480 [00:49<13:52,  1.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 28 examples in round 0.\n",
            "Creating basic bootstrap: 14/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 18/480 [00:19<08:23,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 19 examples in round 0.\n",
            "Creating basic bootstrap: 15/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 20/480 [00:27<10:31,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 21 examples in round 0.\n",
            "Creating basic bootstrap: 16/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 17/480 [00:24<11:07,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 18 examples in round 0.\n",
            "Creating basic bootstrap: 17/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 10/480 [00:15<11:56,  1.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 11 examples in round 0.\n",
            "Creating basic bootstrap: 18/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▎         | 12/480 [00:14<09:16,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 13 examples in round 0.\n",
            "Creating basic bootstrap: 19/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 15/480 [00:26<13:35,  1.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 16 examples in round 0.\n",
            "Creating basic bootstrap: 20/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 27/480 [00:35<09:59,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 28 examples in round 0.\n",
            "Creating basic bootstrap: 21/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 24/480 [00:29<09:14,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 25 examples in round 0.\n",
            "Creating basic bootstrap: 22/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 13/480 [00:24<14:41,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 14 examples in round 0.\n",
            "Creating basic bootstrap: 23/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 15/480 [00:18<09:35,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 16 examples in round 0.\n",
            "Creating basic bootstrap: 24/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 16/480 [00:16<08:05,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 9 full traces after 17 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-07-22 05:24:34,461] A new study created in memory with name: no-name-8d3d3ddb-8a49-43d2-9be5-b8f9c9297b09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model (<dsp.modules.gpt3.GPT3 object at 0x7cf0ebbac9d0>) History:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "You are an instruction optimizer for large language models. I will give you a ``signature`` of fields (inputs and outputs) in English. Specifically, I will give you some ``observations`` I have made about the dataset and task, along with some ``examples`` of the expected inputs and outputs. I will also provide you with the current ``basic instruction`` that is being used for this task.\n",
            "\n",
            "Your task is to propose a new improved instruction and prefix for the output field that will lead a good language model to perform the task well. Don't be afraid to be creative.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Observations: Observations about the dataset and task\n",
            "\n",
            "Examples: Example(s) of the task\n",
            "\n",
            "Basic Instruction: The initial instructions before optimization\n",
            "\n",
            "Proposed Instruction: The improved instructions for the language model\n",
            "\n",
            "Proposed Prefix For Output Field: The string at the end of the prompt, which will help the model start solving the task\n",
            "\n",
            "---\n",
            "\n",
            "Observations: The dataset contains a diverse range of news articles covering various topics, with a focus on conveying factual information in an engaging manner. It includes a mix of biased and non-biased labels, encompassing a variety of viewpoints and perspectives, while maintaining a clear and objective writing style. With headlines that draw readers in and content that is concise and informative, the dataset provides a snapshot of current events from a range of news outlets.\n",
            "\n",
            "Examples:\n",
            "[1] «Base Profile: Age: 53\n",
            "\n",
            "Political Party: Liberal\n",
            "\n",
            "Reasoning: Let's think step by step in order to produce the\n",
            "Demographics Features: Age: 53\n",
            "\n",
            "Gender: Male\n",
            "\n",
            "Political Party: Liberal\n",
            "\n",
            "Country: United States\n",
            "\n",
            "Language: English\n",
            "\n",
            "Base\n",
            "Social Media Features: Facebook Hours: 4\n",
            "Instagram Hours: 3\n",
            "Twitter Hours: 5\n",
            "Reddit Hours: 1\n",
            "News Article Features: News Outlet: FOX\n",
            "\n",
            "Headline: 4 people shot near South By Southwest festival in Texas\n",
            "\n",
            "Content: Fox News\n",
            "Bias Label: is-biased»\n",
            "[2] «Base Profile: A\n",
            "Demographics Features: Age: 25\n",
            "\n",
            "Gender: Female\n",
            "\n",
            "Political Party: Conservative\n",
            "\n",
            "Country: United States\n",
            "\n",
            "Language: English\n",
            "\n",
            "Base\n",
            "Social Media Features: Facebook Hours: 6\n",
            "\n",
            "Instagram Hours: 6\n",
            "\n",
            "Twitter Hours: 5\n",
            "\n",
            "Reddit Hours: 4\n",
            "News Article Features: News Outlet: BBC\n",
            "\n",
            "Headline: Ukraine war: President Zelensky invites Elon Musk to visit\n",
            "\n",
            "Content:\n",
            "Bias Label: is-biased»\n",
            "[3] «Base Profile: A\n",
            "Demographics Features: Age: 26, Gender: Female, Political Party: Independent, Country: United States, Language: English\n",
            "Social Media Features: Facebook Hours: 2\n",
            "Instagram Hours: 3\n",
            "Twitter Hours: 0\n",
            "Reddit Hours: 3\n",
            "News Article Features: News Article Features:\n",
            "- The ongoing struggles faced by the families of Americans detained in Russia, including Brittney Griner,\n",
            "Bias Label: is-biased»\n",
            "[4] «Base Profile: Age: 48\n",
            "\n",
            "Political Party: Liberal\n",
            "\n",
            "Reasoning: Let's think step by step in order to produce the\n",
            "Demographics Features: Age: 48\n",
            "\n",
            "Gender: Male\n",
            "\n",
            "Political Party: Liberal\n",
            "\n",
            "Country: United States\n",
            "\n",
            "Language: English\n",
            "\n",
            "Base\n",
            "Social Media Features: Facebook Hours: 20\n",
            "Instagram Hours: 26\n",
            "Twitter Hours: 15\n",
            "Reddit Hours: 16\n",
            "News Article Features: News Article Features: The news article highlights the importance of vaccination and testing in preventing severe illness and the spread of Covid-\n",
            "Bias Label: is-biased»\n",
            "[5] «Base Profile: A\n",
            "Demographics Features: Age: 42\n",
            "\n",
            "Gender: Male\n",
            "\n",
            "Political Party: Liberal\n",
            "\n",
            "Country: United States\n",
            "\n",
            "Language: English\n",
            "\n",
            "Base\n",
            "Social Media Features: Facebook Hours: 5\n",
            "Instagram Hours: 7\n",
            "Twitter Hours: 4\n",
            "Reddit Hours: 2\n",
            "News Article Features: News Outlet: BBC\n",
            "\n",
            "Headline: Prosecutor to drop murder charge against Texas woman over an abortion\n",
            "\n",
            "Content: A murder\n",
            "Bias Label: is-biased»\n",
            "[6] «Base Profile: A\n",
            "Demographics Features: Age: 25\n",
            "\n",
            "Gender: Female\n",
            "\n",
            "Political Party: Conservative\n",
            "\n",
            "Country: United States\n",
            "\n",
            "Language: English\n",
            "\n",
            "Base\n",
            "Social Media Features: Facebook Hours: 23.0\n",
            "\n",
            "Instagram Hours: 3.0\n",
            "\n",
            "Twitter Hours: 23\n",
            "\n",
            "Reddit\n",
            "News Article Features: News Outlet: BBC\n",
            "\n",
            "Headline: Nicholas Rossi: Man wanted in US on rape charge told to get lawyer\n",
            "\n",
            "Content\n",
            "Bias Label: is-biased»\n",
            "[7] «Base Profile: A\n",
            "Demographics Features: Age: 44\n",
            "\n",
            "Gender: Female\n",
            "\n",
            "Political Party: Conservative\n",
            "\n",
            "Country: United States\n",
            "\n",
            "Language: English\n",
            "\n",
            "Base\n",
            "Social Media Features: Facebook Hours: 5\n",
            "\n",
            "Instagram Hours: 2\n",
            "\n",
            "Twitter Hours: 2\n",
            "\n",
            "Reddit Hours: 2\n",
            "News Article Features: News Outlet: CNN\n",
            "\n",
            "Headline: How this Ukrainian news outlet is protecting its journalists\n",
            "\n",
            "Content: As Russia’s attack\n",
            "Bias Label: is-biased»\n",
            "[8] «Base Profile: Age: 25\n",
            "\n",
            "Political Party: Liberal\n",
            "\n",
            "Reasoning: Let's think step by step in order to produce the\n",
            "Demographics Features: Demographics Features: Young adult male, liberal, from the United Kingdom, English speaker.\n",
            "Social Media Features: Facebook Hours: 2\n",
            "Instagram Hours: 5\n",
            "Twitter Hours: 3\n",
            "Reddit Hours: 2\n",
            "News Article Features: News Article Features: The recent shooting incident in Miami Beach during spring break resulted in chaos and injuries, with two victims transported\n",
            "Bias Label: is-biased»\n",
            "[9] «Base Profile: Age: 41\n",
            "\n",
            "Political Party: Liberal\n",
            "\n",
            "Reasoning: Let's think step by step in order to produce the\n",
            "Demographics Features: Age: 41\n",
            "\n",
            "Gender: Female\n",
            "\n",
            "Political Party: Liberal\n",
            "\n",
            "Country: United States\n",
            "\n",
            "Language: English\n",
            "\n",
            "Base\n",
            "Social Media Features: Facebook Hours: 4\n",
            "Instagram Hours: 4\n",
            "Twitter Hours: 2\n",
            "Reddit Hours: 1\n",
            "News Article Features: News Outlet: CNN\n",
            "\n",
            "Headline: Hatchet-wielding attacker at Canada mosque charged for possible hate-motivated attack\n",
            "Bias Label: is-biased»\n",
            "\n",
            "Basic Instruction: Given the fields `base_profile`, `demographics_features`, `social_media_features`, `news_article_features`, produce the fields `bias_label`.\n",
            "\n",
            "Proposed Instruction:\u001b[32m Examine the `base_profile`, `demographics_features`, `social_media_features`, and `news_article_features` to determine the bias label accurately.\n",
            "\n",
            "Proposed Prefix For Output Field: Identify Bias -\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "Starting trial #0\n",
            "Evaling the following program:\n",
            "Predictor 0\n",
            "i: Given the fields `age` and `political_party`, provide a concise profile description incorporating both fields.\n",
            "p: Description:\n",
            "\n",
            "\n",
            "Predictor 1\n",
            "i: Considering the variegation of the dataset and the need for coherent information representation: derive all fields through simulation from multiple viewpoints\n",
            "p: cerebral_hint2\n",
            "\n",
            "\n",
            "Predictor 2\n",
            "i: Aggregate the provided `facebook_hours`, `instagram_hours`, `twitter_hours`, and `reddit_hours` into a feature set denoted by `social_media_features`.\n",
            "p: Social Media Features:\n",
            "\n",
            "\n",
            "Predictor 3\n",
            "i: Create a detailed analysis of the news article, covering key points such as the election results, candidate statements, voter turnout, polling data, and reactions from other candidates. Include insights on potential implications for French politics and society.\n",
            "p: Generate a comprehensive analysis relating to the election results between Macron and Le Pen, focusing on various aspects such as the candidates' performance, voter sentiments, and future impact on French politics.\n",
            "\n",
            "\n",
            "Predictor 4\n",
            "i: In cognitive bypass\n",
            "p: New Angle:\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 55 / 100  (55.0): 100%|██████████| 100/100 [02:36<00:00,  1.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 55 / 100  (55.0%)\n",
            "0st split score: 55.0\n",
            "curr average score: 55.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 66 / 100  (66.0): 100%|██████████| 100/100 [01:54<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 66 / 100  (66.0%)\n",
            "1st split score: 66.0\n",
            "curr average score: 60.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 67 / 100  (67.0): 100%|██████████| 100/100 [01:44<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 67 / 100  (67.0%)\n",
            "2st split score: 67.0\n",
            "curr average score: 62.666666666666664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 65 / 100  (65.0): 100%|██████████| 100/100 [01:29<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 65 / 100  (65.0%)\n",
            "3st split score: 65.0\n",
            "curr average score: 63.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 49 / 80  (61.2): 100%|██████████| 80/80 [01:08<00:00,  1.16it/s]\n",
            "[I 2024-07-22 05:33:29,326] Trial 0 finished with value: 62.916666666666664 and parameters: {'137380054953184_predictor_instruction': 11, '137380054953184_predictor_demos': 9, '137380054954384_predictor_instruction': 19, '137380054954384_predictor_demos': 13, '137379959436096_predictor_instruction': 12, '137379959436096_predictor_demos': 14, '137379959424192_predictor_instruction': 4, '137379959424192_predictor_demos': 3, '137379959425776_predictor_instruction': 3, '137379959425776_predictor_demos': 1}. Best is trial 0 with value: 62.916666666666664.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 49 / 80  (61.2%)\n",
            "4st split score: 61.25\n",
            "curr average score: 62.916666666666664\n",
            "Fully evaled score: 62.916666666666664\n",
            "Model (<dsp.modules.gpt3.GPT3 object at 0x7cf0ebbaf670>) History:\n",
            "Returning consider_base_profile = ChainOfThought(StringSignature(age, political_party -> base_profile\n",
            "    instructions='Given the fields `age`, `political_party`, produce the fields `base_profile`.'\n",
            "    age = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Age:', 'desc': '${age}'})\n",
            "    political_party = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Political Party:', 'desc': '${political_party}'})\n",
            "    base_profile = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Base Profile:', 'desc': '${base_profile}'})\n",
            "))\n",
            "process_demographics = ChainOfThought(StringSignature(age, gender, political_party, country, language, base_profile -> demographics_features\n",
            "    instructions='Given the fields `age`, `gender`, `political_party`, `country`, `language`, `base_profile`, produce the fields `demographics_features`.'\n",
            "    age = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Age:', 'desc': '${age}'})\n",
            "    gender = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Gender:', 'desc': '${gender}'})\n",
            "    political_party = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Political Party:', 'desc': '${political_party}'})\n",
            "    country = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Country:', 'desc': '${country}'})\n",
            "    language = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Language:', 'desc': '${language}'})\n",
            "    base_profile = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Base Profile:', 'desc': '${base_profile}'})\n",
            "    demographics_features = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Demographics Features:', 'desc': '${demographics_features}'})\n",
            "))\n",
            "process_social_media = ChainOfThought(StringSignature(facebook_hours, instagram_hours, twitter_hours, reddit_hours -> social_media_features\n",
            "    instructions='Given the fields `facebook_hours`, `instagram_hours`, `twitter_hours`, `reddit_hours`, produce the fields `social_media_features`.'\n",
            "    facebook_hours = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Facebook Hours:', 'desc': '${facebook_hours}'})\n",
            "    instagram_hours = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Instagram Hours:', 'desc': '${instagram_hours}'})\n",
            "    twitter_hours = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Twitter Hours:', 'desc': '${twitter_hours}'})\n",
            "    reddit_hours = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Reddit Hours:', 'desc': '${reddit_hours}'})\n",
            "    social_media_features = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Social Media Features:', 'desc': '${social_media_features}'})\n",
            "))\n",
            "process_news_article = ChainOfThought(StringSignature(news_outlet, headline, content -> news_article_features\n",
            "    instructions='Given the fields `news_outlet`, `headline`, `content`, produce the fields `news_article_features`.'\n",
            "    news_outlet = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'News Outlet:', 'desc': '${news_outlet}'})\n",
            "    headline = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Headline:', 'desc': '${headline}'})\n",
            "    content = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Content:', 'desc': '${content}'})\n",
            "    news_article_features = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'News Article Features:', 'desc': '${news_article_features}'})\n",
            "))\n",
            "predict_bias = ChainOfThought(StringSignature(base_profile, demographics_features, social_media_features, news_article_features -> bias_label\n",
            "    instructions='Given the fields `base_profile`, `demographics_features`, `social_media_features`, `news_article_features`, produce the fields `bias_label`.'\n",
            "    base_profile = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Base Profile:', 'desc': '${base_profile}'})\n",
            "    demographics_features = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Demographics Features:', 'desc': '${demographics_features}'})\n",
            "    social_media_features = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Social Media Features:', 'desc': '${social_media_features}'})\n",
            "    news_article_features = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'News Article Features:', 'desc': '${news_article_features}'})\n",
            "    bias_label = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Bias Label:', 'desc': '${bias_label}'})\n",
            ")) from continue_program\n"
          ]
        }
      ],
      "source": [
        "import dspy\n",
        "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, MIPRO, KNNFewShot\n",
        "# Predict the perceived bias of a news article based on the demographics, social media habits, and content of the article. Consider the individual's age, gender, political party affiliation, country of residence, primary language, and daily social media usage on Facebook, Instagram, Twitter, and Reddit, along with the news outlet and the headline/content of the article.\n",
        "class Bias(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Predict whether a news article is perceived as biased or not based on a person's demographics, social media usage, and the article contents.\n",
        "\n",
        "    The input features include:\n",
        "    - age: The age of the participant.\n",
        "    - gender: The gender of the participant.\n",
        "    - political_party: The political party affiliation of the participant.\n",
        "    - country: The country where the participant resides.\n",
        "    - language: The primary language spoken by the participant.\n",
        "    - facebook_hours: The number of hours the participant spends on Facebook per day.\n",
        "    - instagram_hours: The number of hours the participant spends on Instagram per day.\n",
        "    - twitter_hours: The number of hours the participant spends on Twitter per day.\n",
        "    - reddit_hours: The number of hours the participant spends on Reddit per day.\n",
        "    - news_outlet: The news outlet that published the headline.\n",
        "    - headline: The headline of the news article.\n",
        "    - content: The content of the news article.\n",
        "\n",
        "    The output is:\n",
        "    - bias_label: The participant's subjective classification of the headline as 'is-biased' or 'is-not-biased'.\n",
        "    \"\"\"\n",
        "    age = dspy.InputField(desc=\"The age of the participant.\")\n",
        "    gender = dspy.InputField(desc=\"The gender of the participant.\")\n",
        "    political_party = dspy.InputField(desc=\"The political party affiliation of the participant.\")\n",
        "    country = dspy.InputField(desc=\"The country where the participant resides.\")\n",
        "    language = dspy.InputField(desc=\"The primary language spoken by the participant.\")\n",
        "    facebook_hours = dspy.InputField(desc=\"The number of hours the participant spends on Facebook per day.\")\n",
        "    instagram_hours = dspy.InputField(desc=\"The number of hours the participant spends on Instagram per day.\")\n",
        "    twitter_hours = dspy.InputField(desc=\"The number of hours the participant spends on Twitter per day.\")\n",
        "    reddit_hours = dspy.InputField(desc=\"The number of hours the participant spends on Reddit per day.\")\n",
        "    news_outlet = dspy.InputField(desc=\"The news outlet that published the headline.\")\n",
        "    headline = dspy.InputField(desc=\"The headline of the news article.\")\n",
        "    content = dspy.InputField(desc=\"The news story content\")\n",
        "    bias_label = dspy.OutputField(desc=\"The participant's subjective classification of the headline as 'is-biased' or 'is-not-biased'.\")\n",
        "\n",
        "    def validate(self):\n",
        "        assert self.bias_label in ['is-biased', 'is-not-biased'], \"Invalid bias_label value\"\n",
        "\n",
        "# Define a module for bias prediction\n",
        "class BiasPrediction(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # ChainOfThought for processing demographics\n",
        "        self.consider_base_profile = dspy.ChainOfThought(\"age, political_party -> base_profile\")\n",
        "\n",
        "        self.process_demographics = dspy.ChainOfThought(\"age, gender, political_party, country, language, base_profile -> demographics_features\")\n",
        "\n",
        "        # ChainOfThought for processing social media usage\n",
        "        self.process_social_media = dspy.ChainOfThought(\"facebook_hours, instagram_hours, twitter_hours, reddit_hours -> social_media_features\")\n",
        "\n",
        "        # ChainOfThought for processing news article\n",
        "        self.process_news_article = dspy.ChainOfThought(\"news_outlet, headline, content -> news_article_features\")\n",
        "\n",
        "        # ChainOfThought for combining all features and predicting bias\n",
        "        self.predict_bias = dspy.ChainOfThought(\"base_profile, demographics_features, social_media_features, news_article_features -> bias_label\")\n",
        "\n",
        "    def forward(self, age, gender, political_party, country, language, facebook_hours, instagram_hours, twitter_hours, reddit_hours, news_outlet, headline, content):\n",
        "        # Process demographics\n",
        "        base_profile = self.consider_base_profile(\n",
        "            age=age,\n",
        "            political_party=political_party\n",
        "        ).base_profile\n",
        "\n",
        "        demographics_features = self.process_demographics(\n",
        "            age=age,\n",
        "            gender=gender,\n",
        "            political_party=political_party,\n",
        "            country=country,\n",
        "            language=language,\n",
        "            base_profile=base_profile\n",
        "        ).demographics_features\n",
        "\n",
        "        # Process social media usage\n",
        "        social_media_features = self.process_social_media(\n",
        "            facebook_hours=facebook_hours,\n",
        "            instagram_hours=instagram_hours,\n",
        "            twitter_hours=twitter_hours,\n",
        "            reddit_hours=reddit_hours\n",
        "        ).social_media_features\n",
        "\n",
        "        # Process news article\n",
        "        news_article_features = self.process_news_article(\n",
        "            news_outlet=news_outlet,\n",
        "            headline=headline,\n",
        "            content=content\n",
        "        ).news_article_features\n",
        "\n",
        "        # Predict bias using combined features\n",
        "        prediction = self.predict_bias(\n",
        "            base_profile=base_profile,\n",
        "            demographics_features=demographics_features,\n",
        "            social_media_features=social_media_features,\n",
        "            news_article_features=news_article_features\n",
        "        )\n",
        "\n",
        "        return dspy.Prediction(bias_label=prediction.bias_label)\n",
        "\n",
        "train_size = int(0.8 * len(df))\n",
        "\n",
        "# Define a function to create examples from dataframe entries\n",
        "def create_example(entry):\n",
        "    return dspy.Example(\n",
        "        age=entry['Answer.age'],\n",
        "        gender=entry['Answer.gender'],\n",
        "        political_party=entry['Answer.politics'],\n",
        "        country=entry['Answer.country'],\n",
        "        language=entry['Answer.language1'],\n",
        "        facebook_hours=entry['Answer.facebook-hours'],\n",
        "        instagram_hours=entry['Answer.instagram-hours'],\n",
        "        twitter_hours=entry['Answer.twitter-hours'],\n",
        "        reddit_hours=entry['Answer.reddit-hours'],\n",
        "        news_outlet=entry['Answer.newsOutlet'],\n",
        "        headline=entry['headline'],\n",
        "        content=entry['content'],\n",
        "        bias_label=entry['Answer.bias-question']\n",
        "    ).with_inputs(\n",
        "        'age',\n",
        "        'gender',\n",
        "        'country',\n",
        "        'facebook_hours',\n",
        "        'instagram_hours',\n",
        "        'twitter_hours',\n",
        "        'reddit_hours',\n",
        "        'language',\n",
        "        'news_outlet',\n",
        "        'political_party',\n",
        "        'url',\n",
        "        'headline',\n",
        "        'content'\n",
        "    )\n",
        "\n",
        "# Create training and validation sets using list comprehensions\n",
        "trainset = [create_example(entry) for index, entry in df.iloc[:train_size].iterrows()]\n",
        "\n",
        "devset = [create_example(entry) for index, entry in df.iloc[train_size:].iterrows()]\n",
        "\n",
        "\n",
        "print(len(trainset), len(devset))\n",
        "print('still need to consider two datasets segmented on the political party, training two models on the respective political party and then using those model predictions.')\n",
        "\n",
        "\n",
        "def validate_response(example, pred, trace=None):\n",
        "    valid_bias_labels = ['is-biased', 'is-not-biased']\n",
        "    if pred.bias_label not in valid_bias_labels:\n",
        "        return False\n",
        "    return example.bias_label == pred.bias_label\n",
        "\n",
        "\n",
        "# config = dict(max_bootstrapped_demos=4, max_labeled_demos=4, num_candidate_programs=5, num_threads=4)\n",
        "# teleprompter = BootstrapFewShotWithRandomSearch(metric=validate_answer, **config)\n",
        "# config = dict(max_bootstrapped_demos=4, max_labeled_demos=4, num_candidate_programs=10, num_threads=4)\n",
        "\n",
        "# teleprompter = BootstrapFewShotWithRandomSearch(metric=validate_response, **config)\n",
        "# optimized_program = teleprompter.compile(BiasPrediction(), trainset=trainset)\n",
        "\n",
        "# knn_teleprompter = KNNFewShot(KNN, k=7, trainset=trainset)\n",
        "# compiled_knn = knn_teleprompter.compile(BiasPrediction(), trainset=trainset)\n",
        "\n",
        "mipro_config = {\n",
        "    'metric': validate_response,\n",
        "    'prompt_model': dspy.OpenAI(model='gpt-3.5-turbo'),\n",
        "    'task_model': dspy.OpenAI(model='gpt-3.5-turbo'),\n",
        "    'num_candidates': 25,\n",
        "    'init_temperature': 1.5,\n",
        "    'verbose': True,\n",
        "    'track_stats': True,\n",
        "    'view_data_batch_size': 10\n",
        "}\n",
        "eval_kwargs = dict(num_threads=3, display_progress=True, display_table=0)\n",
        "Bayesian_teleprompter = MIPRO(**mipro_config)\n",
        "\n",
        "# Compile the module with the training data\n",
        "# optimized_program = teleprompter.compile(BiasPrediction(), trainset=trainset)\n",
        "bayesian_optimized_program = Bayesian_teleprompter.compile(\n",
        "    BiasPrediction(),\n",
        "    trainset=trainset,\n",
        "    num_trials=5,\n",
        "    max_bootstrapped_demos=9,\n",
        "    max_labeled_demos=5,\n",
        "    eval_kwargs=eval_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "PEA6QX0hk2dX",
        "outputId": "4fddaa88-de36-4692-901c-9a66eb3b2874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 72 / 120  (60.0): 100%|██████████| 120/120 [01:12<00:00,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 72 / 120  (60.0%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cf243de2a40>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_1c5ec th {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_1c5ec td {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_1c5ec_row0_col0, #T_1c5ec_row0_col1, #T_1c5ec_row0_col2, #T_1c5ec_row0_col3, #T_1c5ec_row0_col4, #T_1c5ec_row0_col5, #T_1c5ec_row0_col6, #T_1c5ec_row0_col7, #T_1c5ec_row0_col8, #T_1c5ec_row0_col9, #T_1c5ec_row0_col10, #T_1c5ec_row0_col11, #T_1c5ec_row0_col12, #T_1c5ec_row0_col13, #T_1c5ec_row0_col14, #T_1c5ec_row1_col0, #T_1c5ec_row1_col1, #T_1c5ec_row1_col2, #T_1c5ec_row1_col3, #T_1c5ec_row1_col4, #T_1c5ec_row1_col5, #T_1c5ec_row1_col6, #T_1c5ec_row1_col7, #T_1c5ec_row1_col8, #T_1c5ec_row1_col9, #T_1c5ec_row1_col10, #T_1c5ec_row1_col11, #T_1c5ec_row1_col12, #T_1c5ec_row1_col13, #T_1c5ec_row1_col14, #T_1c5ec_row2_col0, #T_1c5ec_row2_col1, #T_1c5ec_row2_col2, #T_1c5ec_row2_col3, #T_1c5ec_row2_col4, #T_1c5ec_row2_col5, #T_1c5ec_row2_col6, #T_1c5ec_row2_col7, #T_1c5ec_row2_col8, #T_1c5ec_row2_col9, #T_1c5ec_row2_col10, #T_1c5ec_row2_col11, #T_1c5ec_row2_col12, #T_1c5ec_row2_col13, #T_1c5ec_row2_col14, #T_1c5ec_row3_col0, #T_1c5ec_row3_col1, #T_1c5ec_row3_col2, #T_1c5ec_row3_col3, #T_1c5ec_row3_col4, #T_1c5ec_row3_col5, #T_1c5ec_row3_col6, #T_1c5ec_row3_col7, #T_1c5ec_row3_col8, #T_1c5ec_row3_col9, #T_1c5ec_row3_col10, #T_1c5ec_row3_col11, #T_1c5ec_row3_col12, #T_1c5ec_row3_col13, #T_1c5ec_row3_col14, #T_1c5ec_row4_col0, #T_1c5ec_row4_col1, #T_1c5ec_row4_col2, #T_1c5ec_row4_col3, #T_1c5ec_row4_col4, #T_1c5ec_row4_col5, #T_1c5ec_row4_col6, #T_1c5ec_row4_col7, #T_1c5ec_row4_col8, #T_1c5ec_row4_col9, #T_1c5ec_row4_col10, #T_1c5ec_row4_col11, #T_1c5ec_row4_col12, #T_1c5ec_row4_col13, #T_1c5ec_row4_col14 {\n",
              "  text-align: left;\n",
              "  white-space: pre-wrap;\n",
              "  word-wrap: break-word;\n",
              "  max-width: 400px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_1c5ec\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_1c5ec_level0_col0\" class=\"col_heading level0 col0\" >age</th>\n",
              "      <th id=\"T_1c5ec_level0_col1\" class=\"col_heading level0 col1\" >gender</th>\n",
              "      <th id=\"T_1c5ec_level0_col2\" class=\"col_heading level0 col2\" >political_party</th>\n",
              "      <th id=\"T_1c5ec_level0_col3\" class=\"col_heading level0 col3\" >country</th>\n",
              "      <th id=\"T_1c5ec_level0_col4\" class=\"col_heading level0 col4\" >language</th>\n",
              "      <th id=\"T_1c5ec_level0_col5\" class=\"col_heading level0 col5\" >facebook_hours</th>\n",
              "      <th id=\"T_1c5ec_level0_col6\" class=\"col_heading level0 col6\" >instagram_hours</th>\n",
              "      <th id=\"T_1c5ec_level0_col7\" class=\"col_heading level0 col7\" >twitter_hours</th>\n",
              "      <th id=\"T_1c5ec_level0_col8\" class=\"col_heading level0 col8\" >reddit_hours</th>\n",
              "      <th id=\"T_1c5ec_level0_col9\" class=\"col_heading level0 col9\" >news_outlet</th>\n",
              "      <th id=\"T_1c5ec_level0_col10\" class=\"col_heading level0 col10\" >headline</th>\n",
              "      <th id=\"T_1c5ec_level0_col11\" class=\"col_heading level0 col11\" >content</th>\n",
              "      <th id=\"T_1c5ec_level0_col12\" class=\"col_heading level0 col12\" >example_bias_label</th>\n",
              "      <th id=\"T_1c5ec_level0_col13\" class=\"col_heading level0 col13\" >pred_bias_label</th>\n",
              "      <th id=\"T_1c5ec_level0_col14\" class=\"col_heading level0 col14\" >validate_response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_1c5ec_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_1c5ec_row0_col0\" class=\"data row0 col0\" >53</td>\n",
              "      <td id=\"T_1c5ec_row0_col1\" class=\"data row0 col1\" >Female</td>\n",
              "      <td id=\"T_1c5ec_row0_col2\" class=\"data row0 col2\" >Liberal</td>\n",
              "      <td id=\"T_1c5ec_row0_col3\" class=\"data row0 col3\" >United States</td>\n",
              "      <td id=\"T_1c5ec_row0_col4\" class=\"data row0 col4\" >English</td>\n",
              "      <td id=\"T_1c5ec_row0_col5\" class=\"data row0 col5\" >1.0</td>\n",
              "      <td id=\"T_1c5ec_row0_col6\" class=\"data row0 col6\" >3.0</td>\n",
              "      <td id=\"T_1c5ec_row0_col7\" class=\"data row0 col7\" >3</td>\n",
              "      <td id=\"T_1c5ec_row0_col8\" class=\"data row0 col8\" >0</td>\n",
              "      <td id=\"T_1c5ec_row0_col9\" class=\"data row0 col9\" >CNN</td>\n",
              "      <td id=\"T_1c5ec_row0_col10\" class=\"data row0 col10\" >Pentagon says Polish proposal to transfer jets to US to give to Ukraine isn’t ‘tenable’</td>\n",
              "      <td id=\"T_1c5ec_row0_col11\" class=\"data row0 col11\" >The Pentagon on Tuesday evening dismissed Poland’s proposal floated hours earlier to transfer its MiG-29 fighter jets to the United States for delivery to Ukraine....</td>\n",
              "      <td id=\"T_1c5ec_row0_col12\" class=\"data row0 col12\" >is-biased</td>\n",
              "      <td id=\"T_1c5ec_row0_col13\" class=\"data row0 col13\" >is-biased</td>\n",
              "      <td id=\"T_1c5ec_row0_col14\" class=\"data row0 col14\" >✔️ [True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1c5ec_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_1c5ec_row1_col0\" class=\"data row1 col0\" >54</td>\n",
              "      <td id=\"T_1c5ec_row1_col1\" class=\"data row1 col1\" >Female</td>\n",
              "      <td id=\"T_1c5ec_row1_col2\" class=\"data row1 col2\" >Conservative</td>\n",
              "      <td id=\"T_1c5ec_row1_col3\" class=\"data row1 col3\" >United States</td>\n",
              "      <td id=\"T_1c5ec_row1_col4\" class=\"data row1 col4\" >English</td>\n",
              "      <td id=\"T_1c5ec_row1_col5\" class=\"data row1 col5\" >2</td>\n",
              "      <td id=\"T_1c5ec_row1_col6\" class=\"data row1 col6\" >0</td>\n",
              "      <td id=\"T_1c5ec_row1_col7\" class=\"data row1 col7\" >0</td>\n",
              "      <td id=\"T_1c5ec_row1_col8\" class=\"data row1 col8\" >0</td>\n",
              "      <td id=\"T_1c5ec_row1_col9\" class=\"data row1 col9\" >CNN</td>\n",
              "      <td id=\"T_1c5ec_row1_col10\" class=\"data row1 col10\" >10 memorable moments from past Supreme Court nominations</td>\n",
              "      <td id=\"T_1c5ec_row1_col11\" class=\"data row1 col11\" >Supreme Court confirmation hearings were first televised, gavel-to-gavel, when Sandra Day O’Connor appeared before the Senate Judiciary Committee in 1981. The country’s first woman justice,...</td>\n",
              "      <td id=\"T_1c5ec_row1_col12\" class=\"data row1 col12\" >is-not-biased</td>\n",
              "      <td id=\"T_1c5ec_row1_col13\" class=\"data row1 col13\" >is-biased</td>\n",
              "      <td id=\"T_1c5ec_row1_col14\" class=\"data row1 col14\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1c5ec_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_1c5ec_row2_col0\" class=\"data row2 col0\" >67</td>\n",
              "      <td id=\"T_1c5ec_row2_col1\" class=\"data row2 col1\" >Female</td>\n",
              "      <td id=\"T_1c5ec_row2_col2\" class=\"data row2 col2\" >Liberal</td>\n",
              "      <td id=\"T_1c5ec_row2_col3\" class=\"data row2 col3\" >United States</td>\n",
              "      <td id=\"T_1c5ec_row2_col4\" class=\"data row2 col4\" >English</td>\n",
              "      <td id=\"T_1c5ec_row2_col5\" class=\"data row2 col5\" >12</td>\n",
              "      <td id=\"T_1c5ec_row2_col6\" class=\"data row2 col6\" >0</td>\n",
              "      <td id=\"T_1c5ec_row2_col7\" class=\"data row2 col7\" >0</td>\n",
              "      <td id=\"T_1c5ec_row2_col8\" class=\"data row2 col8\" >0</td>\n",
              "      <td id=\"T_1c5ec_row2_col9\" class=\"data row2 col9\" >BBC</td>\n",
              "      <td id=\"T_1c5ec_row2_col10\" class=\"data row2 col10\" >Ghislaine Maxwell: Juror admits 'mistake' not revealing abuse</td>\n",
              "      <td id=\"T_1c5ec_row2_col11\" class=\"data row2 col11\" >A juror in Ghislaine Maxwell's sex abuse trial has told a hearing it was an \"inadvertent mistake\", not a lie, to fail to disclose on...</td>\n",
              "      <td id=\"T_1c5ec_row2_col12\" class=\"data row2 col12\" >is-not-biased</td>\n",
              "      <td id=\"T_1c5ec_row2_col13\" class=\"data row2 col13\" >is-biased</td>\n",
              "      <td id=\"T_1c5ec_row2_col14\" class=\"data row2 col14\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1c5ec_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_1c5ec_row3_col0\" class=\"data row3 col0\" >52</td>\n",
              "      <td id=\"T_1c5ec_row3_col1\" class=\"data row3 col1\" >Male</td>\n",
              "      <td id=\"T_1c5ec_row3_col2\" class=\"data row3 col2\" >Liberal</td>\n",
              "      <td id=\"T_1c5ec_row3_col3\" class=\"data row3 col3\" >United States</td>\n",
              "      <td id=\"T_1c5ec_row3_col4\" class=\"data row3 col4\" >English</td>\n",
              "      <td id=\"T_1c5ec_row3_col5\" class=\"data row3 col5\" >3</td>\n",
              "      <td id=\"T_1c5ec_row3_col6\" class=\"data row3 col6\" >0</td>\n",
              "      <td id=\"T_1c5ec_row3_col7\" class=\"data row3 col7\" >0</td>\n",
              "      <td id=\"T_1c5ec_row3_col8\" class=\"data row3 col8\" >0</td>\n",
              "      <td id=\"T_1c5ec_row3_col9\" class=\"data row3 col9\" >CNN</td>\n",
              "      <td id=\"T_1c5ec_row3_col10\" class=\"data row3 col10\" >Hatchet-wielding attacker at Canada mosque charged for possible hate-motivated attack, police say</td>\n",
              "      <td id=\"T_1c5ec_row3_col11\" class=\"data row3 col11\" >The man who allegedly discharged bear spray while brandishing a hatchet at a mosque in the Canadian city of Mississauga is now facing multiple charges...</td>\n",
              "      <td id=\"T_1c5ec_row3_col12\" class=\"data row3 col12\" >is-not-biased</td>\n",
              "      <td id=\"T_1c5ec_row3_col13\" class=\"data row3 col13\" >is-biased</td>\n",
              "      <td id=\"T_1c5ec_row3_col14\" class=\"data row3 col14\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1c5ec_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_1c5ec_row4_col0\" class=\"data row4 col0\" >38</td>\n",
              "      <td id=\"T_1c5ec_row4_col1\" class=\"data row4 col1\" >Female</td>\n",
              "      <td id=\"T_1c5ec_row4_col2\" class=\"data row4 col2\" >Liberal</td>\n",
              "      <td id=\"T_1c5ec_row4_col3\" class=\"data row4 col3\" >United States</td>\n",
              "      <td id=\"T_1c5ec_row4_col4\" class=\"data row4 col4\" >English</td>\n",
              "      <td id=\"T_1c5ec_row4_col5\" class=\"data row4 col5\" >0</td>\n",
              "      <td id=\"T_1c5ec_row4_col6\" class=\"data row4 col6\" >9</td>\n",
              "      <td id=\"T_1c5ec_row4_col7\" class=\"data row4 col7\" >0</td>\n",
              "      <td id=\"T_1c5ec_row4_col8\" class=\"data row4 col8\" >0</td>\n",
              "      <td id=\"T_1c5ec_row4_col9\" class=\"data row4 col9\" >FOX</td>\n",
              "      <td id=\"T_1c5ec_row4_col10\" class=\"data row4 col10\" >West Texas wildfires prompt evacuations amid dry, windy conditions</td>\n",
              "      <td id=\"T_1c5ec_row4_col11\" class=\"data row4 col11\" >Fox News Flash top headlines are here. Check out what's clicking on Foxnews.com. Wildfires ripping through West Texas this week prompted evacuations on Thursday and...</td>\n",
              "      <td id=\"T_1c5ec_row4_col12\" class=\"data row4 col12\" >is-not-biased</td>\n",
              "      <td id=\"T_1c5ec_row4_col13\" class=\"data row4 col13\" >is-biased</td>\n",
              "      <td id=\"T_1c5ec_row4_col14\" class=\"data row4 col14\" >False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                <div style='\n",
              "                    text-align: center;\n",
              "                    font-size: 16px;\n",
              "                    font-weight: bold;\n",
              "                    color: #555;\n",
              "                    margin: 10px 0;'>\n",
              "                    ... 115 more rows not displayed ...\n",
              "                </div>\n",
              "                "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation score: 60.0\n"
          ]
        }
      ],
      "source": [
        "from dspy.evaluate import Evaluate\n",
        "# Set up the evaluator\n",
        "evaluate_on_devset = Evaluate(devset=devset, num_threads=4, display_progress=True, display_table=5)\n",
        "\n",
        "# Evaluate the optimized program\n",
        "eval_score = evaluate_on_devset(bayesian_optimized_program, metric=validate_response)\n",
        "\n",
        "# Print the evaluation score\n",
        "print(f\"Evaluation score: {eval_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au6b9DJgZimj"
      },
      "outputs": [],
      "source": [
        "bayesian_optimized_program.save('/content/bayesian_optimized_program600_1trials_eval_score60.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne-UWR5jhDGS",
        "outputId": "43ddf409-6f6d-494b-e10d-bf2c78fe3b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Independent' 'Liberal' 'Conservative' 'Other' 'default']\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('cleaned_data_processed.csv')\n",
        "\n",
        "df['Answer.politics'].unique\n",
        "\n",
        "n=1500\n",
        "random_seed = 42\n",
        "df = df.sample(n=n, random_state=random_seed)\n",
        "print(df['Answer.politics'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viqOFSKaaHXR"
      },
      "outputs": [],
      "source": [
        "import dspy\n",
        "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, MIPRO\n",
        "# Predict the perceived bias of a news article based on the demographics, social media habits, and content of the article. Consider the individual's age, gender, political party affiliation, country of residence, primary language, and daily social media usage on Facebook, Instagram, Twitter, and Reddit, along with the news outlet and the headline/content of the article.\n",
        "class Bias(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Predict whether a news article is perceived as biased or not based on a person's demographics, social media usage, and the article contents.\n",
        "\n",
        "    The input features include:\n",
        "    - age: The age of the participant.\n",
        "    - gender: The gender of the participant.\n",
        "    - political_party: The political party affiliation of the participant.\n",
        "    - country: The country where the participant resides.\n",
        "    - language: The primary language spoken by the participant.\n",
        "    - facebook_hours: The number of hours the participant spends on Facebook per day.\n",
        "    - instagram_hours: The number of hours the participant spends on Instagram per day.\n",
        "    - twitter_hours: The number of hours the participant spends on Twitter per day.\n",
        "    - reddit_hours: The number of hours the participant spends on Reddit per day.\n",
        "    - news_outlet: The news outlet that published the headline.\n",
        "    - headline: The headline of the news article.\n",
        "    - content: The content of the news article.\n",
        "\n",
        "    The output is:\n",
        "    - bias_label: The participant's subjective classification of the headline as 'is-biased' or 'is-not-biased'.\n",
        "    \"\"\"\n",
        "    age = dspy.InputField(desc=\"The age of the participant.\")\n",
        "    gender = dspy.InputField(desc=\"The gender of the participant.\")\n",
        "    political_party = dspy.InputField(desc=\"The political party affiliation of the participant.\")\n",
        "    country = dspy.InputField(desc=\"The country where the participant resides.\")\n",
        "    language = dspy.InputField(desc=\"The primary language spoken by the participant.\")\n",
        "    facebook_hours = dspy.InputField(desc=\"The number of hours the participant spends on Facebook per day.\")\n",
        "    instagram_hours = dspy.InputField(desc=\"The number of hours the participant spends on Instagram per day.\")\n",
        "    twitter_hours = dspy.InputField(desc=\"The number of hours the participant spends on Twitter per day.\")\n",
        "    reddit_hours = dspy.InputField(desc=\"The number of hours the participant spends on Reddit per day.\")\n",
        "    news_outlet = dspy.InputField(desc=\"The news outlet that published the headline.\")\n",
        "    headline = dspy.InputField(desc=\"The headline of the news article.\")\n",
        "    content = dspy.InputField(desc=\"The news story content\")\n",
        "    bias_label = dspy.OutputField(desc=\"The participant's subjective classification of the headline as either 'is-biased' or 'is-not-biased'.\")\n",
        "\n",
        "# Define a module for bias prediction\n",
        "class BiasPrediction(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.initial_profiling = dspy.ChainOfThought(\"age, political_party -> political_profile\")\n",
        "        self.national_origin = dspy.ChainOfThought(\"country, langauge -> cultural_background\")\n",
        "\n",
        "        # ChainOfThought for processing demographics\n",
        "        self.process_demographics = dspy.ChainOfThought(\"profile, gender, cultural_background -> demographics_features\")\n",
        "\n",
        "        # ChainOfThought for processing social media usage\n",
        "        self.process_social_media = dspy.ChainOfThought(\"facebook_hours, instagram_hours, twitter_hours, reddit_hours -> social_media_features\")\n",
        "\n",
        "        # ChainOfThought for processing news article\n",
        "        self.process_news_article = dspy.ChainOfThought(\"news_outlet, headline, content -> news_article_features\")\n",
        "\n",
        "        # ChainOfThought for combining all features and predicting bias\n",
        "        self.predict_bias = dspy.ChainOfThought(\"demographics_features, social_media_features, news_article_features -> bias_label\")\n",
        "\n",
        "    def forward(self, age, gender, political_party, country, language, facebook_hours, instagram_hours, twitter_hours, reddit_hours, news_outlet, headline, content):\n",
        "        # Process demographics\n",
        "        demographics_features = self.process_demographics(\n",
        "            age=age,\n",
        "            gender=gender,\n",
        "            political_party=political_party,\n",
        "            country=country,\n",
        "            language=language\n",
        "        ).demographics_features\n",
        "\n",
        "        # Process social media usage\n",
        "        social_media_features = self.process_social_media(\n",
        "            facebook_hours=facebook_hours,\n",
        "            instagram_hours=instagram_hours,\n",
        "            twitter_hours=twitter_hours,\n",
        "            reddit_hours=reddit_hours\n",
        "        ).social_media_features\n",
        "\n",
        "        # Process news article\n",
        "        news_article_features = self.process_news_article(\n",
        "            news_outlet=news_outlet,\n",
        "            headline=headline,\n",
        "            content=content\n",
        "        ).news_article_features\n",
        "\n",
        "        # Predict bias using combined features\n",
        "        prediction = self.predict_bias(\n",
        "            demographics_features=demographics_features,\n",
        "            social_media_features=social_media_features,\n",
        "            news_article_features=news_article_features\n",
        "        )\n",
        "\n",
        "        return dspy.Prediction(bias_label=prediction.bias_label)\n",
        "\n",
        "train_size = int(0.8 * len(df))\n",
        "\n",
        "# Define a function to create examples from dataframe entries\n",
        "def create_example(entry):\n",
        "    return dspy.Example(\n",
        "        age=entry['Answer.age'],\n",
        "        gender=entry['Answer.gender'],\n",
        "        political_party=entry['Answer.politics'],\n",
        "        country=entry['Answer.country'],\n",
        "        language=entry['Answer.language1'],\n",
        "        facebook_hours=entry['Answer.facebook-hours'],\n",
        "        instagram_hours=entry['Answer.instagram-hours'],\n",
        "        twitter_hours=entry['Answer.twitter-hours'],\n",
        "        reddit_hours=entry['Answer.reddit-hours'],\n",
        "        news_outlet=entry['Answer.newsOutlet'],\n",
        "        headline=entry['headline'],\n",
        "        content=entry['content'],\n",
        "        bias_label=entry['Answer.bias-question']\n",
        "    ).with_inputs(\n",
        "        'age',\n",
        "        'gender',\n",
        "        'country',\n",
        "        'facebook_hours',\n",
        "        'instagram_hours',\n",
        "        'twitter_hours',\n",
        "        'reddit_hours',\n",
        "        'language',\n",
        "        'news_outlet',\n",
        "        'political_party',\n",
        "        'url',\n",
        "        'headline',\n",
        "        'content'\n",
        "    )\n",
        "\n",
        "# Create training and validation sets using list comprehensions\n",
        "trainset = [create_example(entry) for index, entry in df.iloc[:train_size].iterrows()]\n",
        "\n",
        "devset = [create_example(entry) for index, entry in df.iloc[train_size:].iterrows()]\n",
        "\n",
        "\n",
        "print(len(trainset), len(devset))\n",
        "print('still need to consider two datasets segmented on the political party, training two models on the respective political party and then using those model predictions.')\n",
        "\n",
        "\n",
        "def validate_response(example, pred, trace=None):\n",
        "    return dspy.evaluate.answer_exact_match(example.bias_label, pred.bias_label)\n",
        "\n",
        "\n",
        "\n",
        "# config = dict(max_bootstrapped_demos=4, max_labeled_demos=4, num_candidate_programs=5, num_threads=4)\n",
        "\n",
        "# teleprompter = BootstrapFewShotWithRandomSearch(metric=validate_answer, **config)\n",
        "\n",
        "mipro_config = {\n",
        "    'metric': validate_answer,\n",
        "    'prompt_model': dspy.OpenAI(model='gpt-3.5-turbo'),\n",
        "    'task_model': dspy.OpenAI(model='gpt-3.5-turbo'),\n",
        "    'num_candidates': 10,\n",
        "    'init_temperature': 1.0,\n",
        "    'verbose': True,\n",
        "    'track_stats': True,\n",
        "    'view_data_batch_size': 10\n",
        "}\n",
        "eval_kwargs = dict(num_threads=3, display_progress=True, display_table=0)\n",
        "Bayesian_teleprompter = MIPRO(**mipro_config)\n",
        "\n",
        "# Compile the module with the training data\n",
        "# optimized_program = teleprompter.compile(BiasPrediction(), trainset=trainset)\n",
        "bayesian_optimized_program = Bayesian_teleprompter.compile(BiasPrediction(), trainset=trainset, num_trials=3, max_bootstrapped_demos=1, max_labeled_demos=2, eval_kwargs=eval_kwargs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN6umGCdE2ueEY85jp6eIe8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}